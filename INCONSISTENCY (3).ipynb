{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89WwIEkw1gzs"
      },
      "source": [
        "CONNECTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z1HfLx6O5fw"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvjbTV9bYJwB",
        "outputId": "0338715a-4f1b-42d8-a4d1-975f270f3e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.26\" 2025-01-21\n",
            "OpenJDK Runtime Environment (build 11.0.26+4-post-Ubuntu-1ubuntu122.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.26+4-post-Ubuntu-1ubuntu122.04, mixed mode, sharing)\n"
          ]
        }
      ],
      "source": [
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zqi2xsNZoMV",
        "outputId": "0da8c03f-8fd7-4b31-eaa5-8a26641b1f1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,762 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,677 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Fetched 24.3 MB in 9s (2,703 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common librsvg2-common libxkbfile1\n",
            "  libxt-dev libxtst6 libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre openjdk-8-jre-headless\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libxt-doc openjdk-8-demo openjdk-8-source visualvm libnss-mdns fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common librsvg2-common libxkbfile1\n",
            "  libxt-dev libxtst6 libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 20 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 50.1 MB of archives.\n",
            "After this operation, 169 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u442-b06~us1-0ubuntu1~22.04 [30.8 MB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre amd64 8u442-b06~us1-0ubuntu1~22.04 [75.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk-headless amd64 8u442-b06~us1-0ubuntu1~22.04 [8,864 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk amd64 8u442-b06~us1-0ubuntu1~22.04 [4,077 kB]\n",
            "Fetched 50.1 MB in 2s (24.9 MB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../04-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../06-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../07-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../08-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../09-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../10-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../11-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../12-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../13-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../14-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../15-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../16-openjdk-8-jre-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../17-openjdk-8-jre_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../18-openjdk-8-jdk-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../19-openjdk-8-jdk_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUfa0ZPraQGJ",
        "outputId": "a51d82c4-b445-402e-c273-c477a2f93cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-21 05:17:16--  https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M  55.3MB/s    in 4.5s    \n",
            "\n",
            "2025-03-21 05:17:20 (48.4 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "qcIFYXGGsNB4",
        "outputId": "7c5bacd6-76a2-4d32-9b6a-a97dfdae72b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 223476\n",
            "drwxr-xr-x 1 root root      4096 Mar 19 13:34 sample_data\n",
            "-rw-r--r-- 1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "total 223480\n",
            "drwxr-xr-x  1 root root      4096 Mar 19 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24  2021 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "SPARK_HOME is set to: /content/spark-3.1.2-bin-hadoop3.2\n",
            "Spark session created successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b8581752990>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8756577f8417:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>TestSpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "\n",
        "# Step 2: Extract Spark\n",
        "!ls -l\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "# Step 3: Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "# Step 4: Initialize findspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "print(\"SPARK_HOME is set to:\", os.environ[\"SPARK_HOME\"])\n",
        "\n",
        "# Step 5: Verify Spark Installation\n",
        "!pip install -q pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"TestSpark\").getOrCreate()\n",
        "print(\"Spark session created successfully!\")\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUkvIAwksZBy",
        "outputId": "a06d3a1c-411b-4976-d1e6-b83fb85a24d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access your datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkRWtlEX1sGv"
      },
      "source": [
        "SAMPLE FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCrYgP0Ustxd",
        "outputId": "275db3fe-7a38-4915-bd11-10b63a0e7099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample CSV generated successfully:\n",
            "   id                                        text_column\n",
            "0   1                             This is a sample text.\n",
            "1   2                      This text is another example.\n",
            "2   3            Sample text with duplicate sample text.\n",
            "3   4                   Another row with unique content.\n",
            "4   5  This text contains some noise, e.g., punctuation!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create sample data with an 'id' column and a 'text_column'\n",
        "data_dict = {\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'text_column': [\n",
        "        \"This is a sample text.\",\n",
        "        \"This text is another example.\",\n",
        "        \"Sample text with duplicate sample text.\",\n",
        "        \"Another row with unique content.\",\n",
        "        \"This text contains some noise, e.g., punctuation!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = pd.DataFrame(data_dict)\n",
        "\n",
        "# Save the DataFrame to a CSV file named 'sample.csv' in the current directory\n",
        "csv_file_path = 'sample.csv'\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Display the DataFrame and confirm the CSV file generation\n",
        "print(\"Sample CSV generated successfully:\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqp41zyM1ut7"
      },
      "source": [
        "DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LbbEsl6sfxG",
        "outputId": "d357c91e-1a0f-4bab-9bb0-779161aff38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data from CSV:\n",
            "   id                                        text_column\n",
            "0   1                             This is a sample text.\n",
            "1   2                      This text is another example.\n",
            "2   3            Sample text with duplicate sample text.\n",
            "3   4                   Another row with unique content.\n",
            "4   5  This text contains some noise, e.g., punctuation!\n"
          ]
        }
      ],
      "source": [
        "# Import pandas to work with CSV files\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to your CSV file stored in Google Drive\n",
        "csv_file_path = '/content/sample.csv'  # update this path if needed\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the first few rows to verify the data\n",
        "print(\"Sample data from CSV:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U0R3sVFtM8D",
        "outputId": "90bd36d9-843d-4d65-c336-492467eb01a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "# Download the missing punkt_tab resource\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ysj8M5ks3Nq",
        "outputId": "5767bf7a-fcc0-436a-e6ca-3ae11e648118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after tokenization:\n",
            "   id                                        text_column  \\\n",
            "0   1                             This is a sample text.   \n",
            "1   2                      This text is another example.   \n",
            "2   3            Sample text with duplicate sample text.   \n",
            "3   4                   Another row with unique content.   \n",
            "4   5  This text contains some noise, e.g., punctuation!   \n",
            "\n",
            "                                              tokens  \n",
            "0                     [this, is, a, sample, text, .]  \n",
            "1              [this, text, is, another, example, .]  \n",
            "2   [sample, text, with, duplicate, sample, text, .]  \n",
            "3           [another, row, with, unique, content, .]  \n",
            "4  [this, text, contains, some, noise, ,, e.g., ,...  \n"
          ]
        }
      ],
      "source": [
        "# Import nltk and download the tokenizer models\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define a function to clean and tokenize text\n",
        "def clean_and_tokenize(text):\n",
        "    # Convert to lowercase and strip leading/trailing whitespace\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Assuming the CSV has a column named 'text_column'\n",
        "# Apply the cleaning and tokenization to create a new 'tokens' column\n",
        "data['tokens'] = data['text_column'].apply(clean_and_tokenize)\n",
        "\n",
        "# Display the updated DataFrame with the new 'tokens' column\n",
        "print(\"Data after tokenization:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymljYeh7tfo8",
        "outputId": "30ef6625-05c8-437c-9743-e215f0bf3124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   id                                        text_column\n",
            "0   1                             This is a sample text.\n",
            "1   2                      This text is another example.\n",
            "2   3            Sample text with duplicate sample text.\n",
            "3   4                   Another row with unique content.\n",
            "4   5  This text contains some noise, e.g., punctuation! \n",
            "\n",
            "Detected Duplicate Entries:\n",
            "Empty DataFrame\n",
            "Columns: [id, text_column]\n",
            "Index: [] \n",
            "\n",
            "Data with Inconsistency Issues Detected:\n",
            "   id                                        text_column formatting_issues  \\\n",
            "0   1                             This is a sample text.                []   \n",
            "1   2                      This text is another example.                []   \n",
            "2   3            Sample text with duplicate sample text.                []   \n",
            "3   4                   Another row with unique content.                []   \n",
            "4   5  This text contains some noise, e.g., punctuation!                []   \n",
            "\n",
            "  conflicting_information data_anomalies  \n",
            "0                      []             []  \n",
            "1                      []             []  \n",
            "2                      []             []  \n",
            "3                      []             []  \n",
            "4                      []             []  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load sample CSV file (make sure sample.csv exists in the current directory)\n",
        "data = pd.read_csv('sample.csv')\n",
        "print(\"Original Data:\")\n",
        "print(data, \"\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Duplicate Content Detection\n",
        "# -------------------------------\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "duplicates_df = detect_duplicates(data)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Formatting Errors Detection\n",
        "# -------------------------------\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "    - Starts with an uppercase letter.\n",
        "    - Ends with proper punctuation (., !, or ?).\n",
        "\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Conflicting Information Detection\n",
        "# -------------------------------\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Example conflict pairs include:\n",
        "      - ('increase', 'decrease')\n",
        "      - ('up', 'down')\n",
        "      - ('true', 'false')\n",
        "      - ('positive', 'negative')\n",
        "      - ('win', 'loss')\n",
        "      - ('hot', 'cold')\n",
        "\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Data Anomalies Detection\n",
        "# -------------------------------\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "    - Empty text.\n",
        "    - Text that is extremely short.\n",
        "    - Excessive punctuation (three or more punctuation marks in a row).\n",
        "    - A high ratio of non-alphanumeric characters compared to alphanumeric ones.\n",
        "\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    # Detect excessive punctuation (3+ consecutive punctuation marks)\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    # Check for a high ratio of non-alphanumeric to alphanumeric characters\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "# -------------------------------\n",
        "# Apply All Detection Functions\n",
        "# -------------------------------\n",
        "# Apply formatting, conflicting information, and data anomalies detectors\n",
        "data['formatting_issues'] = data['text_column'].apply(detect_formatting_issues)\n",
        "data['conflicting_information'] = data['text_column'].apply(detect_conflicting_information)\n",
        "data['data_anomalies'] = data['text_column'].apply(detect_data_anomalies)\n",
        "\n",
        "# -------------------------------\n",
        "# Output the Results\n",
        "# -------------------------------\n",
        "print(\"Detected Duplicate Entries:\")\n",
        "print(duplicates_df, \"\\n\")\n",
        "\n",
        "print(\"Data with Inconsistency Issues Detected:\")\n",
        "print(data[['id', 'text_column', 'formatting_issues', 'conflicting_information', 'data_anomalies']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyAaNnCW12s9"
      },
      "source": [
        "TESTING WITH CSV FILE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ae2cc5bd81f343978de96913bcc764f1",
            "4d21dde36d084f32822c5d821fe89f99",
            "5a627967389a466d8e3c8641d6adcb6d",
            "d1e0c8a7d1c742dabf5c2e7e9484ea87",
            "8d9de0405247408397b7689f6fc5cc63",
            "1495b8eeb77c4f8cbdfb7bb1e0c45fea",
            "a36095fe5c644fab991e7e56cf96c17a",
            "76f6f8564d584e87b1e67d82b225fc94",
            "c710e8e495b24a279c02a17fef0b7a4a",
            "25d67930ad8e49a18cc95e2ea4f24ef0"
          ]
        },
        "id": "8MaiIbqPu9DI",
        "outputId": "63a9cc29-8ae6-4d52-b015-e1549f972029"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(FileUpload(value={}, accept='.csv', description='Upload'), Button(description='Process Document…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae2cc5bd81f343978de96913bcc764f1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, FileLink, clear_output\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    # Capitalize first letter if not already\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    # Append period if missing ending punctuation\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Build the Upload & Processing Interface\n",
        "# -------------------------------\n",
        "\n",
        "# Create a file upload widget (accepting only CSV files)\n",
        "upload_widget = widgets.FileUpload(accept='.csv', multiple=False)\n",
        "\n",
        "# Create a process button widget\n",
        "process_button = widgets.Button(description=\"Process Document\")\n",
        "\n",
        "# Output area to display results and download link\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def process_file(button):\n",
        "    with output_area:\n",
        "        clear_output()  # Clear previous output\n",
        "        if upload_widget.value:\n",
        "            # Get the uploaded file (only one file allowed)\n",
        "            uploaded_filename = list(upload_widget.value.keys())[0]\n",
        "            content = upload_widget.value[uploaded_filename]['content']\n",
        "            # Read CSV from the uploaded file content\n",
        "            df = pd.read_csv(io.BytesIO(content))\n",
        "\n",
        "            print(\"Original Data:\")\n",
        "            print(df.head(), \"\\n\")\n",
        "\n",
        "            # Apply inconsistency detection on the 'text_column'\n",
        "            df['formatting_issues'] = df['text_column'].apply(detect_formatting_issues)\n",
        "            df['conflicting_information'] = df['text_column'].apply(detect_conflicting_information)\n",
        "            df['data_anomalies'] = df['text_column'].apply(detect_data_anomalies)\n",
        "\n",
        "            # Check for duplicate content\n",
        "            duplicates_df = detect_duplicates(df)\n",
        "            print(\"Detected Duplicate Entries:\")\n",
        "            if duplicates_df.empty:\n",
        "                print(\"No duplicate entries found.\\n\")\n",
        "            else:\n",
        "                print(duplicates_df, \"\\n\")\n",
        "\n",
        "            # Create a corrected text column for basic formatting corrections\n",
        "            df['corrected_text'] = df['text_column'].apply(correct_text)\n",
        "\n",
        "            print(\"Processed Data with Inconsistency Report:\")\n",
        "            display(df[['text_column', 'formatting_issues', 'conflicting_information', 'data_anomalies', 'corrected_text']])\n",
        "\n",
        "            # Save the corrected document to CSV\n",
        "            corrected_filename = \"corrected_document.csv\"\n",
        "            df.to_csv(corrected_filename, index=False)\n",
        "            print(\"\\nDownload the corrected document:\")\n",
        "            display(FileLink(corrected_filename))\n",
        "        else:\n",
        "            print(\"No file uploaded. Please upload a CSV file.\")\n",
        "\n",
        "# Bind the process_file function to the button click event\n",
        "process_button.on_click(process_file)\n",
        "\n",
        "# Display the interface: file upload widget, process button, and output area\n",
        "display(widgets.VBox([upload_widget, process_button, output_area]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz8acMRD6ycX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230d02ae-334f-4c63-9660-6ade0f65b9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvdh9iBYyR_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "a45eb1f9-a81d-41c4-e443-fabb358d03c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e9d4a5d67316591e9d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-62aab80662d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;31m# Launch with debug turned off (set debug=True for additional logs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2793\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnetworking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2794\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m                     artifact = HTML(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36murl_ok\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             if (\n\u001b[1;32m     70\u001b[0m                 \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m302\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m303\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m307\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mHEAD\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     return request(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ) as client:\n\u001b[0;32m--> 109\u001b[0;31m         return client.request(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import subprocess\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"hdfs\", \"dfs\", \"-put\", \"-f\", local_file, hdfs_path],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"Failed to upload to HDFS: {e.stderr.decode('utf-8')}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Handle file object: if it's a dict, use the 'name' key.\n",
        "    file_path = file_obj.name if hasattr(file_obj, \"name\") else file_obj[\"name\"]\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "        elif ext == \".txt\":\n",
        "            spark_df = spark.read.text(file_path)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext == \".json\":\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_path)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        elif ext == \".xml\":\n",
        "            try:\n",
        "                df = pd.read_xml(file_path)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"  # Adjust based on your Hadoop config\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    analysis_text = (\n",
        "        f\"Analysis Summary:\\n\"\n",
        "        f\"Total records: {total_records}\\n\"\n",
        "        f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "        f\"Breakdown of inconsistency types:\\n\"\n",
        "    )\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    if issues_counts:\n",
        "        x_vals = list(issues_counts.keys())\n",
        "        y_vals = list(issues_counts.values())\n",
        "        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')\n",
        "        ax.set_title(\"Trend of Detected Inconsistencies\")\n",
        "        ax.set_xlabel(\"Inconsistency Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',\n",
        "                verticalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_output = gr.Plot(label=\"Inconsistencies Trend Graph\")\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]\n",
        "    )\n",
        "\n",
        "# Launch with debug turned off (set debug=True for additional logs)\n",
        "demo.launch(share=True, debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcmcmigIpTUZ"
      },
      "outputs": [],
      "source": [
        "pip install gradio pandas matplotlib pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ww3B_mJbiVm",
        "outputId": "20368a22-bd53-4c87-c73b-12b157d3ae91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d2d5831b1a7e58f9d8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    Wrapped in try/except to avoid stopping the pipeline if HDFS isn't available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        command = f\"hdfs dfs -put -f {local_file} {hdfs_path}\"\n",
        "        os.system(command)\n",
        "        return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Hadoop upload failed: {e}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "def process_file(file_obj):\n",
        "    # Check if a file is provided\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"No file uploaded.\", None\n",
        "\n",
        "    # Extract the file path from the uploaded file object.\n",
        "    # Gradio returns a dict with a \"name\" key.\n",
        "    file_path = file_obj[\"name\"] if isinstance(file_obj, dict) else file_obj.name\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "            # Check for required column in CSV file\n",
        "            if \"text_column\" not in spark_df.columns:\n",
        "                return pd.DataFrame({\"Error\": [\"CSV file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"CSV missing 'text_column'.\", None\n",
        "        elif ext == \".txt\":\n",
        "            spark_df = spark.read.text(file_path)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext == \".json\":\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_path)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"JSON missing 'text_column'.\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, f\"Error reading JSON: {e}\", None\n",
        "        elif ext == \".xml\":\n",
        "            try:\n",
        "                df = pd.read_xml(file_path)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"XML missing 'text_column'.\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, f\"Error reading XML: {e}\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"Unsupported file format.\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, f\"Error reading file: {e}\", None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    # Adjust the HDFS path based on your Hadoop configuration; if Hadoop is not configured,\n",
        "    # this function will simply return an error message without halting processing.\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    analysis_text = f\"Analysis Summary:\\nTotal records: {total_records}\\n\"\n",
        "    analysis_text += f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "    analysis_text += \"Breakdown of inconsistency types:\\n\"\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    if issues_counts:\n",
        "        x_vals = list(issues_counts.keys())\n",
        "        y_vals = list(issues_counts.values())\n",
        "        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')\n",
        "        ax.set_title(\"Trend of Detected Inconsistencies\")\n",
        "        ax.set_xlabel(\"Inconsistency Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',\n",
        "                verticalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            # This widget provides the download link for the corrected CSV file.\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_output = gr.Plot(label=\"Inconsistencies Trend Graph\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, inline=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twzK_E7_KMNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795185bd-dec2-45ba-b62c-e9092d1d624b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4feb192d3f35151a2f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    command = f\"hdfs dfs -put -f {local_file} {hdfs_path}\"\n",
        "    os.system(command)\n",
        "    return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Start execution timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    ext = os.path.splitext(file_obj.name)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_obj.name)\n",
        "        elif ext in [\".txt\"]:\n",
        "            spark_df = spark.read.text(file_obj.name)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext in [\".json\"]:\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_obj.name)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_obj.name, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        elif ext in [\".xml\"]:\n",
        "            try:\n",
        "                df = pd.read_xml(file_obj.name)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "\n",
        "    # Calculate accuracy rate as the percentage of records without issues\n",
        "    accuracy_rate = ((total_records - records_with_errors) / total_records * 100) if total_records > 0 else 0\n",
        "\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    # End execution timer and compute processing time\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    analysis_text = f\"Analysis Summary:\\nTotal records: {total_records}\\n\"\n",
        "    analysis_text += f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "    analysis_text += f\"Accuracy Rate: {accuracy_rate:.2f}%\\n\"\n",
        "    analysis_text += f\"Execution Time: {execution_time:.2f} seconds\\n\"\n",
        "    analysis_text += \"Breakdown of inconsistency types:\\n\"\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    # Simulate scale-up: Projected execution times for increasing dataset sizes.\n",
        "    # (These factors are for demonstration purposes.)\n",
        "    scale_factors = [1, 2, 3, 4, 5]\n",
        "    simulated_times = [execution_time * factor * 0.9 for factor in scale_factors]\n",
        "\n",
        "    # Create subplots for Accuracy Rate, Execution Time, and Scale-Up Analysis\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Graph 1: Accuracy Rate\n",
        "    axs[0].bar([\"Accuracy Rate\"], [accuracy_rate], color='green')\n",
        "    axs[0].set_ylim(0, 100)\n",
        "    axs[0].set_title(\"Accuracy Rate\")\n",
        "    axs[0].set_ylabel(\"Percentage (%)\")\n",
        "\n",
        "    # Graph 2: Execution Time\n",
        "    axs[1].bar([\"Execution Time\"], [execution_time], color='blue')\n",
        "    axs[1].set_title(\"Execution Time\")\n",
        "    axs[1].set_ylabel(\"Time (seconds)\")\n",
        "\n",
        "    # Graph 3: Scale-Up Analysis\n",
        "    axs[2].plot(scale_factors, simulated_times, marker='o', linestyle='-', color='orange')\n",
        "    axs[2].set_title(\"Scale-Up Analysis\")\n",
        "    axs[2].set_xlabel(\"Scale Factor (Dataset Size Multiplier)\")\n",
        "    axs[2].set_ylabel(\"Simulated Execution Time (s)\")\n",
        "    axs[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_output = gr.Plot(label=\"Performance Metrics Graphs\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, inline=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66kjsrXjvPWm"
      },
      "source": [
        "Code to Generate Document with Inconsistencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvwUWUeGvVos",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "a39f8829-b5a9-4606-8ec5-a2b530f7c233"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-73e0b26ad9c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgradio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    command = f\"hdfs dfs -put -f {local_file} {hdfs_path}\"\n",
        "    os.system(command)\n",
        "    return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None, None, None\n",
        "\n",
        "    # Start execution timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    ext = os.path.splitext(file_obj.name)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_obj.name)\n",
        "        elif ext in [\".txt\"]:\n",
        "            spark_df = spark.read.text(file_obj.name)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext in [\".json\"]:\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_obj.name)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_obj.name, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "        elif ext in [\".xml\"]:\n",
        "            try:\n",
        "                df = pd.read_xml(file_obj.name)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "\n",
        "    # Calculate accuracy rate as the percentage of records without issues\n",
        "    accuracy_rate = ((total_records - records_with_errors) / total_records * 100) if total_records > 0 else 0\n",
        "\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    # End execution timer and compute processing time\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    analysis_text = f\"Analysis Summary:\\nTotal records: {total_records}\\n\"\n",
        "    analysis_text += f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "    analysis_text += f\"Accuracy Rate: {accuracy_rate:.2f}%\\n\"\n",
        "    analysis_text += f\"Execution Time: {execution_time:.2f} seconds\\n\"\n",
        "    analysis_text += \"Breakdown of inconsistency types:\\n\"\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    # Simulate scale-up: Projected execution times for increasing dataset sizes.\n",
        "    # (These factors are for demonstration purposes.)\n",
        "    scale_factors = [1, 2, 3, 4, 5]\n",
        "    simulated_times = [execution_time * factor * 0.9 for factor in scale_factors]\n",
        "\n",
        "    # -------------------------------\n",
        "    # Create Individual Graphs\n",
        "    # -------------------------------\n",
        "\n",
        "    # Graph 1: Accuracy Rate\n",
        "    fig_accuracy, ax1 = plt.subplots(figsize=(6, 4))\n",
        "    ax1.bar([\"Accuracy Rate\"], [accuracy_rate], color='green')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.set_title(\"Accuracy Rate\")\n",
        "    ax1.set_ylabel(\"Percentage (%)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Graph 2: Execution Time\n",
        "    fig_execution, ax2 = plt.subplots(figsize=(6, 4))\n",
        "    ax2.bar([\"Execution Time\"], [execution_time], color='blue')\n",
        "    ax2.set_title(\"Execution Time\")\n",
        "    ax2.set_ylabel(\"Time (seconds)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Graph 3: Scale-Up Analysis\n",
        "    fig_scaleup, ax3 = plt.subplots(figsize=(6, 4))\n",
        "    ax3.plot(scale_factors, simulated_times, marker='o', linestyle='-', color='orange')\n",
        "    ax3.set_title(\"Scale-Up Analysis\")\n",
        "    ax3.set_xlabel(\"Scale Factor (Dataset Size Multiplier)\")\n",
        "    ax3.set_ylabel(\"Simulated Execution Time (s)\")\n",
        "    ax3.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig_accuracy, fig_execution, fig_scaleup\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_accuracy = gr.Plot(label=\"Accuracy Rate\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_execution = gr.Plot(label=\"Execution Time\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_scaleup = gr.Plot(label=\"Scale-Up Analysis\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary,\n",
        "                 plot_accuracy, plot_execution, plot_scaleup]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, inline=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75VOmtrar6e3"
      },
      "source": [
        "generate document with inconsistencies upto 100 lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJhMgx9Rrj2n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# Define categories of inconsistencies\n",
        "conflict_pairs = [\n",
        "    (\"increase\", \"decrease\"), (\"up\", \"down\"), (\"true\", \"false\"),\n",
        "    (\"positive\", \"negative\"), (\"win\", \"loss\"), (\"hot\", \"cold\")\n",
        "]\n",
        "\n",
        "formatting_issues = [\n",
        "    \"this sentence lacks capitalization.\",  # No capitalization\n",
        "    \"Missing punctuation at the end\",  # No punctuation\n",
        "    \"Excessive punctuation!!!\",  # Too much punctuation\n",
        "    \"Short\"  # Too short\n",
        "]\n",
        "\n",
        "duplicates = [\n",
        "    \"Repeated text entry for testing purposes.\",\n",
        "    \"Duplicated line appearing multiple times in dataset.\"\n",
        "]\n",
        "\n",
        "# Generate 100 rows of inconsistent data\n",
        "data = []\n",
        "for i in range(1, 101):\n",
        "    inconsistency_type = random.choice([\"format\", \"conflict\", \"duplicate\"])\n",
        "\n",
        "    if inconsistency_type == \"format\":\n",
        "        text = random.choice(formatting_issues)\n",
        "    elif inconsistency_type == \"conflict\":\n",
        "        pair = random.choice(conflict_pairs)\n",
        "        text = f\"{pair[0]} and {pair[1]} appear together.\"\n",
        "    else:  # duplicate\n",
        "        text = random.choice(duplicates)\n",
        "\n",
        "    data.append({\"id\": i, \"text_column\": text})\n",
        "\n",
        "# Create DataFrame\n",
        "df_inconsistent = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "csv_filename = \"inconsistent_sample_100.csv\"\n",
        "df_inconsistent.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Display sample rows\n",
        "print(\"Generated dataset with inconsistencies:\")\n",
        "print(df_inconsistent.head(10))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import subprocess\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"hdfs\", \"dfs\", \"-put\", \"-f\", local_file, hdfs_path],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"Failed to upload to HDFS: {e.stderr.decode('utf-8')}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Handle file object: if it's a dict, use the 'name' key.\n",
        "    file_path = file_obj.name if hasattr(file_obj, \"name\") else file_obj[\"name\"]\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "        elif ext == \".txt\":\n",
        "            spark_df = spark.read.text(file_path)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext == \".json\":\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_path)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        elif ext == \".xml\":\n",
        "            try:\n",
        "                df = pd.read_xml(file_path)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"  # Adjust based on your Hadoop config\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    analysis_text = (\n",
        "        f\"Analysis Summary:\\n\"\n",
        "        f\"Total records: {total_records}\\n\"\n",
        "        f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "        f\"Breakdown of inconsistency types:\\n\"\n",
        "    )\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    if issues_counts:\n",
        "        x_vals = list(issues_counts.keys())\n",
        "        y_vals = list(issues_counts.values())\n",
        "        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')\n",
        "        ax.set_title(\"Trend of Detected Inconsistencies\")\n",
        "        ax.set_xlabel(\"Inconsistency Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',\n",
        "                verticalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_output = gr.Plot(label=\"Inconsistencies Trend Graph\")\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]\n",
        "    )\n",
        "\n",
        "# Launch with debug=True for Colab to display errors and logs\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "4k9D2fOOD1KX",
        "outputId": "9fe77232-aad9-4114-fdc8-4be4b5fda14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e6255faa3cd326bff8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c94dc7f68247>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;31m# Launch with debug=True for Colab to display errors and logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, share_server_tls_certificate, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2792\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2793\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnetworking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2794\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m                     artifact = HTML(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36murl_ok\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             if (\n\u001b[1;32m     70\u001b[0m                 \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m302\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m303\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m307\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mHEAD\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     return request(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ) as client:\n\u001b[0;32m--> 109\u001b[0;31m         return client.request(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "import subprocess\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, FileLink, clear_output\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"hdfs\", \"dfs\", \"-put\", \"-f\", local_file, hdfs_path],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"Failed to upload to HDFS: {e.stderr.decode('utf-8')}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Define Pandas UDFs for Spark Processing\n",
        "# -------------------------------\n",
        "\n",
        "@pandas_udf(ArrayType(StringType()))\n",
        "def detect_formatting_udf(s: pd.Series) -> pd.Series:\n",
        "    return s.apply(lambda x: detect_formatting_issues(x))\n",
        "\n",
        "@pandas_udf(ArrayType(StringType()))\n",
        "def detect_conflict_udf(s: pd.Series) -> pd.Series:\n",
        "    return s.apply(lambda x: detect_conflicting_information(x))\n",
        "\n",
        "@pandas_udf(ArrayType(StringType()))\n",
        "def detect_anomalies_udf(s: pd.Series) -> pd.Series:\n",
        "    return s.apply(lambda x: detect_data_anomalies(x))\n",
        "\n",
        "@pandas_udf(StringType())\n",
        "def correct_text_udf(s: pd.Series) -> pd.Series:\n",
        "    return s.apply(lambda x: correct_text(x))\n",
        "\n",
        "# -------------------------------\n",
        "# ipywidgets Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Handle file object: if it's a dict, use the 'name' key.\n",
        "    file_path = file_obj.name if hasattr(file_obj, \"name\") else file_obj[\"name\"]\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "        elif ext == \".txt\":\n",
        "            spark_df = spark.read.text(file_path)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext == \".json\":\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_path)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        elif ext == \".xml\":\n",
        "            try:\n",
        "                df = pd.read_xml(file_path)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Apply the Pandas UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"  # Adjust based on your Hadoop config\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    analysis_text = (\n",
        "        f\"Analysis Summary:\\n\"\n",
        "        f\"Total records: {total_records}\\n\"\n",
        "        f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "        f\"Breakdown of inconsistency types:\\n\"\n",
        "    )\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    if issues_counts:\n",
        "        x_vals = list(issues_counts.keys())\n",
        "        y_vals = list(issues_counts.values())\n",
        "        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')\n",
        "        ax.set_title(\"Trend of Detected Inconsistencies\")\n",
        "        ax.set_xlabel(\"Inconsistency Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',\n",
        "                verticalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# ipywidgets User Interface Setup\n",
        "# -------------------------------\n",
        "\n",
        "# Inject custom CSS into the notebook for a dark theme look.\n",
        "display(HTML(\"\"\"\n",
        "<style>\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".widget-box {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "</style>\n",
        "\"\"\"))\n",
        "\n",
        "# Create the file upload widget (accepting CSV, TXT, JSON, XML)\n",
        "upload_widget = widgets.FileUpload(accept=\".csv,.txt,.json,.xml\", multiple=False, description=\"Upload File\")\n",
        "\n",
        "# Create the process button widget\n",
        "process_button = widgets.Button(description=\"Process Document\", button_style='info')\n",
        "\n",
        "# Create output areas for the various outputs\n",
        "download_link = widgets.HTML()  # For displaying download link for corrected CSV\n",
        "processed_preview_output = widgets.Output()\n",
        "duplicate_report_output = widgets.Output()\n",
        "analysis_summary_output = widgets.Output()\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "# Layout the UI using VBox and HBox containers\n",
        "header = widgets.HTML(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "description = widgets.HTML(\n",
        "    \"<p>Upload a file containing a column named <b>'text_column'</b>. \"\n",
        "    \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "    \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "    \"and then produces a corrected file for download. \"\n",
        "    \"Supported formats: CSV, TXT, JSON, XML.</p>\"\n",
        ")\n",
        "file_controls = widgets.HBox([upload_widget, process_button])\n",
        "data_outputs = widgets.HBox([processed_preview_output, duplicate_report_output])\n",
        "result_outputs = widgets.HBox([analysis_summary_output, plot_output])\n",
        "ui = widgets.VBox([header, description, file_controls, download_link, data_outputs, result_outputs])\n",
        "\n",
        "# Define the callback for the process button click event\n",
        "def on_process_button_clicked(b):\n",
        "    # Clear previous outputs\n",
        "    with processed_preview_output:\n",
        "        clear_output()\n",
        "    with duplicate_report_output:\n",
        "        clear_output()\n",
        "    with analysis_summary_output:\n",
        "        clear_output()\n",
        "    with plot_output:\n",
        "        clear_output()\n",
        "    download_link.value = \"\"\n",
        "\n",
        "    # Check if a file has been uploaded\n",
        "    if not upload_widget.value:\n",
        "        with analysis_summary_output:\n",
        "            print(\"Please upload a file first.\")\n",
        "        return\n",
        "\n",
        "    # Retrieve the uploaded file (ipywidgets returns a dict of files)\n",
        "    file_info = list(upload_widget.value.values())[0]\n",
        "    # Get the file name from metadata; fallback to key name if necessary\n",
        "    file_name = file_info['metadata']['name'] if 'metadata' in file_info and 'name' in file_info['metadata'] else file_info['name']\n",
        "\n",
        "    # Write the uploaded file content to disk\n",
        "    with open(file_name, 'wb') as f:\n",
        "        f.write(file_info['content'])\n",
        "\n",
        "    # Create a file object (dictionary with key 'name') as expected by process_file\n",
        "    file_obj = {\"name\": file_name}\n",
        "\n",
        "    # Process the file and get the outputs\n",
        "    processed_preview, duplicate_report, corrected_filename, analysis_text, fig = process_file(file_obj)\n",
        "\n",
        "    # Display the processed preview DataFrame\n",
        "    with processed_preview_output:\n",
        "        print(\"Processed Data Preview:\")\n",
        "        display(processed_preview)\n",
        "\n",
        "    # Display the duplicate report DataFrame\n",
        "    with duplicate_report_output:\n",
        "        print(\"Duplicate Report:\")\n",
        "        display(duplicate_report)\n",
        "\n",
        "    # Display the analysis summary text\n",
        "    with analysis_summary_output:\n",
        "        print(analysis_text)\n",
        "\n",
        "    # Display the matplotlib figure\n",
        "    with plot_output:\n",
        "        display(fig)\n",
        "\n",
        "    # Provide a download link for the corrected CSV file using FileLink\n",
        "    download_link.value = f'<a href=\"{corrected_filename}\" target=\"_blank\">Download Corrected CSV</a>'\n",
        "\n",
        "# Link the button click to the callback function\n",
        "process_button.on_click(on_process_button_clicked)\n",
        "\n",
        "# Display the entire UI\n",
        "display(ui)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b5dc948669bd421bae786464162385eb",
            "5644d08b9a404b40b431ab4db48c5718",
            "ef0b813616d949f996bdbbdc6e469c07",
            "2cee9929c6644db0b8c2726228c9cd6f",
            "503602e557084e5db1589d893e2923b1",
            "6016c22eef2249669e77e25b56dccea6",
            "1a2fc768a1aa4ee3883a4aae9b6018bc",
            "713af917381b455584de0cd8cedac1af",
            "6ae8c999a2484740a50579e22a9a4726",
            "9932332f9f2346e98eaa2c0af995549a",
            "73dd5ae3be6543718875bbe76a169475",
            "d6213fa46d23422ea2da1d0631cf68a4",
            "019bb3d40db545ca8d2a3d32a678c2e9",
            "e78895669c23447ab40306c2225c36b0",
            "feb2f8a1a30142abbf0d7f1d5d8a6a41",
            "cd10495ee24c47b18a94a353095bd9f8",
            "113ac760178143af8173113688b01201",
            "73b23704ea86416faf083d2c007fe4f0",
            "546f7f1596474667a3b8b6fed62f0b40",
            "c4b91a5e28774174801a17141c902abf",
            "f979e2a7be5942718ede86cce6ae71e7",
            "e97245f27b31490cb176e86b30f99209",
            "a37762812dda47d595eac85d3fde580b",
            "3d7392d8356b404f80a386df829922e4",
            "4afff7aed03c4a908d8facedc31eb9c6",
            "8cd1b8ba9e5b4e08a4d780474c211b72",
            "e0762497b9d5466e8a387c8ee88994d3",
            "57ea01126ec1463a8c28f45f4adcad35",
            "4cb6e7df3a7740bcb3c213d4fa2a7b33",
            "271b814198824ef6aba3a1f02c45efb2",
            "75f7c246c3b4487bb100e041441b612b"
          ]
        },
        "id": "nX0TFcUTKUUn",
        "outputId": "cecad89f-0a8b-4a59-8ffd-ffc70c388084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "body {\n",
              "  background-color: #000;\n",
              "  color: #fff;\n",
              "}\n",
              ".widget-box {\n",
              "  border: 1px solid #444;\n",
              "  border-radius: 10px;\n",
              "  padding: 20px;\n",
              "  background-color: #222;\n",
              "  margin: 10px;\n",
              "}\n",
              "h1, h2, h3, h4, h5, h6 {\n",
              "  color: #fff;\n",
              "  text-align: center;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>'), HTML(value…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5dc948669bd421bae786464162385eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\", line 437, in dumps\n",
            "    return cloudpickle.dumps(obj, pickle_protocol)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 630, in reducer_override\n",
            "    return self._function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 503, in _function_reduce\n",
            "    return self._dynamic_function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
            "    state = _function_getstate(func)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 156, in _function_getstate\n",
            "    f_globals_ref = _extract_code_globals(func.__code__)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 247, in _extract_code_globals\n",
            "    out_names |= _extract_code_globals(const)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                 ~~~~~^^^^^^^\n",
            "IndexError: tuple index out of range\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not serialize object: IndexError: tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mreducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mnewargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_getnewargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_function_getstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         return (types.FunctionType, newargs, state, None, None,\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mf_globals_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_code_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCodeType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0mout_names\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0m_extract_code_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e7afe7592d3d>\u001b[0m in \u001b[0;36mon_process_button_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;31m# Process the file and get the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mprocessed_preview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicate_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrected_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# Display the processed preview DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e7afe7592d3d>\u001b[0m in \u001b[0;36mprocess_file\u001b[0;34m(file_obj)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Apply the Pandas UDFs to create new columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"formatting_issues\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_formatting_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_column\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conflicting_information\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_conflict_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_column\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mspark_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_anomalies\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_anomalies_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_column\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# and should have a minimal performance impact.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_judf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_create_judf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mwrapped_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mjdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseDataType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturnType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         judf = sc._jvm.org.apache.spark.sql.execution.python.UserDefinedPythonFunction(\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, returnType)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturnType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m     36\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae2cc5bd81f343978de96913bcc764f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d21dde36d084f32822c5d821fe89f99",
              "IPY_MODEL_5a627967389a466d8e3c8641d6adcb6d",
              "IPY_MODEL_d1e0c8a7d1c742dabf5c2e7e9484ea87"
            ],
            "layout": "IPY_MODEL_8d9de0405247408397b7689f6fc5cc63"
          }
        },
        "4d21dde36d084f32822c5d821fe89f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_1495b8eeb77c4f8cbdfb7bb1e0c45fea",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_a36095fe5c644fab991e7e56cf96c17a"
          }
        },
        "5a627967389a466d8e3c8641d6adcb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Process Document",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_76f6f8564d584e87b1e67d82b225fc94",
            "style": "IPY_MODEL_c710e8e495b24a279c02a17fef0b7a4a",
            "tooltip": ""
          }
        },
        "d1e0c8a7d1c742dabf5c2e7e9484ea87": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_25d67930ad8e49a18cc95e2ea4f24ef0",
            "msg_id": "",
            "outputs": []
          }
        },
        "8d9de0405247408397b7689f6fc5cc63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1495b8eeb77c4f8cbdfb7bb1e0c45fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a36095fe5c644fab991e7e56cf96c17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "76f6f8564d584e87b1e67d82b225fc94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c710e8e495b24a279c02a17fef0b7a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "25d67930ad8e49a18cc95e2ea4f24ef0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5dc948669bd421bae786464162385eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5644d08b9a404b40b431ab4db48c5718",
              "IPY_MODEL_ef0b813616d949f996bdbbdc6e469c07",
              "IPY_MODEL_2cee9929c6644db0b8c2726228c9cd6f",
              "IPY_MODEL_503602e557084e5db1589d893e2923b1",
              "IPY_MODEL_6016c22eef2249669e77e25b56dccea6",
              "IPY_MODEL_1a2fc768a1aa4ee3883a4aae9b6018bc"
            ],
            "layout": "IPY_MODEL_713af917381b455584de0cd8cedac1af"
          }
        },
        "5644d08b9a404b40b431ab4db48c5718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ae8c999a2484740a50579e22a9a4726",
            "placeholder": "​",
            "style": "IPY_MODEL_9932332f9f2346e98eaa2c0af995549a",
            "value": "<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>"
          }
        },
        "ef0b813616d949f996bdbbdc6e469c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73dd5ae3be6543718875bbe76a169475",
            "placeholder": "​",
            "style": "IPY_MODEL_d6213fa46d23422ea2da1d0631cf68a4",
            "value": "<p>Upload a file containing a column named <b>'text_column'</b>. This tool uses Apache Spark for data processing and Hadoop for file storage. It detects formatting issues, conflicting information, and data anomalies, and then produces a corrected file for download. Supported formats: CSV, TXT, JSON, XML.</p>"
          }
        },
        "2cee9929c6644db0b8c2726228c9cd6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_019bb3d40db545ca8d2a3d32a678c2e9",
              "IPY_MODEL_e78895669c23447ab40306c2225c36b0"
            ],
            "layout": "IPY_MODEL_feb2f8a1a30142abbf0d7f1d5d8a6a41"
          }
        },
        "503602e557084e5db1589d893e2923b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd10495ee24c47b18a94a353095bd9f8",
            "placeholder": "​",
            "style": "IPY_MODEL_113ac760178143af8173113688b01201",
            "value": ""
          }
        },
        "6016c22eef2249669e77e25b56dccea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73b23704ea86416faf083d2c007fe4f0",
              "IPY_MODEL_546f7f1596474667a3b8b6fed62f0b40"
            ],
            "layout": "IPY_MODEL_c4b91a5e28774174801a17141c902abf"
          }
        },
        "1a2fc768a1aa4ee3883a4aae9b6018bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f979e2a7be5942718ede86cce6ae71e7",
              "IPY_MODEL_e97245f27b31490cb176e86b30f99209"
            ],
            "layout": "IPY_MODEL_a37762812dda47d595eac85d3fde580b"
          }
        },
        "713af917381b455584de0cd8cedac1af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae8c999a2484740a50579e22a9a4726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9932332f9f2346e98eaa2c0af995549a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73dd5ae3be6543718875bbe76a169475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6213fa46d23422ea2da1d0631cf68a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "019bb3d40db545ca8d2a3d32a678c2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv,.txt,.json,.xml",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload File",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_3d7392d8356b404f80a386df829922e4",
            "metadata": [
              {
                "name": "inconsistent_sample (1).csv",
                "type": "text/csv",
                "size": 294,
                "lastModified": 1742455431709
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_4afff7aed03c4a908d8facedc31eb9c6"
          }
        },
        "e78895669c23447ab40306c2225c36b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Process Document",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8cd1b8ba9e5b4e08a4d780474c211b72",
            "style": "IPY_MODEL_e0762497b9d5466e8a387c8ee88994d3",
            "tooltip": ""
          }
        },
        "feb2f8a1a30142abbf0d7f1d5d8a6a41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd10495ee24c47b18a94a353095bd9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113ac760178143af8173113688b01201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73b23704ea86416faf083d2c007fe4f0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_57ea01126ec1463a8c28f45f4adcad35",
            "msg_id": "",
            "outputs": []
          }
        },
        "546f7f1596474667a3b8b6fed62f0b40": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4cb6e7df3a7740bcb3c213d4fa2a7b33",
            "msg_id": "",
            "outputs": []
          }
        },
        "c4b91a5e28774174801a17141c902abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f979e2a7be5942718ede86cce6ae71e7": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_271b814198824ef6aba3a1f02c45efb2",
            "msg_id": "",
            "outputs": []
          }
        },
        "e97245f27b31490cb176e86b30f99209": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_75f7c246c3b4487bb100e041441b612b",
            "msg_id": "",
            "outputs": []
          }
        },
        "a37762812dda47d595eac85d3fde580b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d7392d8356b404f80a386df829922e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4afff7aed03c4a908d8facedc31eb9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8cd1b8ba9e5b4e08a4d780474c211b72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0762497b9d5466e8a387c8ee88994d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "57ea01126ec1463a8c28f45f4adcad35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb6e7df3a7740bcb3c213d4fa2a7b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "271b814198824ef6aba3a1f02c45efb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75f7c246c3b4487bb100e041441b612b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}