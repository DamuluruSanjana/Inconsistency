# -*- coding: utf-8 -*-
"""INCONSISTENCY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XbClcAhlOtfktqr6ermVdpTXdg3UWJuG

CONNECTION
"""

!pip install -q pyspark findspark

!java -version

!apt-get update
!apt-get install openjdk-8-jdk -y

# Step 1: Download Spark
!wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz

# Step 2: Extract Spark
!ls -l
!tar xf spark-3.1.2-bin-hadoop3.2.tgz
!ls -l

# Step 3: Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop3.2"

# Step 4: Initialize findspark
!pip install -q findspark
import findspark
findspark.init()
print("SPARK_HOME is set to:", os.environ["SPARK_HOME"])

# Step 5: Verify Spark Installation
!pip install -q pyspark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").appName("TestSpark").getOrCreate()
print("Spark session created successfully!")
spark

# Mount Google Drive to access your datasets
from google.colab import drive
drive.mount('/content/drive')

"""SAMPLE FILE"""

import pandas as pd

# Create sample data with an 'id' column and a 'text_column'
data_dict = {
    'id': [1, 2, 3, 4, 5],
    'text_column': [
        "This is a sample text.",
        "This text is another example.",
        "Sample text with duplicate sample text.",
        "Another row with unique content.",
        "This text contains some noise, e.g., punctuation!"
    ]
}

# Create a DataFrame from the sample data
df = pd.DataFrame(data_dict)

# Save the DataFrame to a CSV file named 'sample.csv' in the current directory
csv_file_path = 'sample.csv'
df.to_csv(csv_file_path, index=False)

# Display the DataFrame and confirm the CSV file generation
print("Sample CSV generated successfully:")
print(df)

"""DATA PROCESSING"""

# Import pandas to work with CSV files
import pandas as pd

# Set the path to your CSV file stored in Google Drive
csv_file_path = '/content/sample.csv'  # update this path if needed

# Read the CSV file into a DataFrame
data = pd.read_csv(csv_file_path)

# Display the first few rows to verify the data
print("Sample data from CSV:")
print(data.head())

import nltk
# Download the missing punkt_tab resource
nltk.download('punkt_tab')

# Import nltk and download the tokenizer models
import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize

# Define a function to clean and tokenize text
def clean_and_tokenize(text):
    # Convert to lowercase and strip leading/trailing whitespace
    text = text.lower().strip()
    # Tokenize the text into words
    tokens = word_tokenize(text)
    return tokens

# Assuming the CSV has a column named 'text_column'
# Apply the cleaning and tokenization to create a new 'tokens' column
data['tokens'] = data['text_column'].apply(clean_and_tokenize)

# Display the updated DataFrame with the new 'tokens' column
print("Data after tokenization:")
print(data.head())

import pandas as pd
import re

# Load sample CSV file (make sure sample.csv exists in the current directory)
data = pd.read_csv('sample.csv')
print("Original Data:")
print(data, "\n")

# -------------------------------
# 1. Duplicate Content Detection
# -------------------------------
def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

duplicates_df = detect_duplicates(data)

# -------------------------------
# 2. Formatting Errors Detection
# -------------------------------
def detect_formatting_issues(text):
    """
    Checks whether the text:
    - Starts with an uppercase letter.
    - Ends with proper punctuation (., !, or ?).

    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

# -------------------------------
# 3. Conflicting Information Detection
# -------------------------------
def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Example conflict pairs include:
      - ('increase', 'decrease')
      - ('up', 'down')
      - ('true', 'false')
      - ('positive', 'negative')
      - ('win', 'loss')
      - ('hot', 'cold')

    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

# -------------------------------
# 4. Data Anomalies Detection
# -------------------------------
def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
    - Empty text.
    - Text that is extremely short.
    - Excessive punctuation (three or more punctuation marks in a row).
    - A high ratio of non-alphanumeric characters compared to alphanumeric ones.

    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    # Detect excessive punctuation (3+ consecutive punctuation marks)
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    # Check for a high ratio of non-alphanumeric to alphanumeric characters
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

# -------------------------------
# Apply All Detection Functions
# -------------------------------
# Apply formatting, conflicting information, and data anomalies detectors
data['formatting_issues'] = data['text_column'].apply(detect_formatting_issues)
data['conflicting_information'] = data['text_column'].apply(detect_conflicting_information)
data['data_anomalies'] = data['text_column'].apply(detect_data_anomalies)

# -------------------------------
# Output the Results
# -------------------------------
print("Detected Duplicate Entries:")
print(duplicates_df, "\n")

print("Data with Inconsistency Issues Detected:")
print(data[['id', 'text_column', 'formatting_issues', 'conflicting_information', 'data_anomalies']])

"""TESTING WITH CSV FILE"""

import pandas as pd
import re
import io
import ipywidgets as widgets
from IPython.display import display, FileLink, clear_output

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    # Capitalize first letter if not already
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    # Append period if missing ending punctuation
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Build the Upload & Processing Interface
# -------------------------------

# Create a file upload widget (accepting only CSV files)
upload_widget = widgets.FileUpload(accept='.csv', multiple=False)

# Create a process button widget
process_button = widgets.Button(description="Process Document")

# Output area to display results and download link
output_area = widgets.Output()

def process_file(button):
    with output_area:
        clear_output()  # Clear previous output
        if upload_widget.value:
            # Get the uploaded file (only one file allowed)
            uploaded_filename = list(upload_widget.value.keys())[0]
            content = upload_widget.value[uploaded_filename]['content']
            # Read CSV from the uploaded file content
            df = pd.read_csv(io.BytesIO(content))

            print("Original Data:")
            print(df.head(), "\n")

            # Apply inconsistency detection on the 'text_column'
            df['formatting_issues'] = df['text_column'].apply(detect_formatting_issues)
            df['conflicting_information'] = df['text_column'].apply(detect_conflicting_information)
            df['data_anomalies'] = df['text_column'].apply(detect_data_anomalies)

            # Check for duplicate content
            duplicates_df = detect_duplicates(df)
            print("Detected Duplicate Entries:")
            if duplicates_df.empty:
                print("No duplicate entries found.\n")
            else:
                print(duplicates_df, "\n")

            # Create a corrected text column for basic formatting corrections
            df['corrected_text'] = df['text_column'].apply(correct_text)

            print("Processed Data with Inconsistency Report:")
            display(df[['text_column', 'formatting_issues', 'conflicting_information', 'data_anomalies', 'corrected_text']])

            # Save the corrected document to CSV
            corrected_filename = "corrected_document.csv"
            df.to_csv(corrected_filename, index=False)
            print("\nDownload the corrected document:")
            display(FileLink(corrected_filename))
        else:
            print("No file uploaded. Please upload a CSV file.")

# Bind the process_file function to the button click event
process_button.on_click(process_file)

# Display the interface: file upload widget, process button, and output area
display(widgets.VBox([upload_widget, process_button, output_area]))

!pip install gradio

import os
import json
import pandas as pd
import re
import gradio as gr
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType
import subprocess

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Hadoop Integration Function
# -------------------------------

def upload_to_hdfs(local_file, hdfs_path):
    """
    Uploads a local file to HDFS using a shell command.
    Requires Hadoop to be installed and HDFS running.
    """
    try:
        result = subprocess.run(
            ["hdfs", "dfs", "-put", "-f", local_file, hdfs_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        return f"File uploaded to HDFS at: {hdfs_path}"
    except subprocess.CalledProcessError as e:
        return f"Failed to upload to HDFS: {e.stderr.decode('utf-8')}"

# -------------------------------
# Gradio Interface Processing Function with Spark & Hadoop
# -------------------------------

def process_file(file_obj):
    if file_obj is None:
        return pd.DataFrame(), pd.DataFrame(), None, "", None

    # Handle file object: if it's a dict, use the 'name' key.
    file_path = file_obj.name if hasattr(file_obj, "name") else file_obj["name"]
    ext = os.path.splitext(file_path)[1].lower()

    # Initialize Spark session
    spark = SparkSession.builder.master("local[*]").appName("InconsistencyDetection").getOrCreate()

    try:
        if ext == ".csv":
            spark_df = spark.read.option("header", "true").csv(file_path)
        elif ext == ".txt":
            spark_df = spark.read.text(file_path)
            spark_df = spark_df.withColumnRenamed("value", "text_column")
        elif ext == ".json":
            try:
                spark_df = spark.read.json(file_path)
                if "text_column" not in spark_df.columns:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list) and all(isinstance(x, str) for x in data):
                        spark_df = spark.createDataFrame([(x,) for x in data], ["text_column"])
                    else:
                        return pd.DataFrame({"Error": ["JSON file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading JSON file: {e}"]}), pd.DataFrame(), None, "", None
        elif ext == ".xml":
            try:
                df = pd.read_xml(file_path)
                if 'text_column' not in df.columns:
                    return pd.DataFrame({"Error": ["XML file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
                spark_df = spark.createDataFrame(df)
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading XML file: {e}"]}), pd.DataFrame(), None, "", None
        else:
            return pd.DataFrame({"Error": ["Unsupported file format. Supported formats: CSV, TXT, JSON, XML."]}), pd.DataFrame(), None, "", None
    except Exception as e:
        return pd.DataFrame({"Error": [f"Error reading file: {e}"]}), pd.DataFrame(), None, "", None

    # Define Spark UDFs wrapping our Python functions
    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))
    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))
    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))
    correct_text_udf = udf(lambda x: correct_text(x), StringType())

    # Apply UDFs to create new columns
    spark_df = spark_df.withColumn("formatting_issues", detect_formatting_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("conflicting_information", detect_conflict_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("data_anomalies", detect_anomalies_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("corrected_text", correct_text_udf(spark_df["text_column"]))

    # Convert the Spark DataFrame to a Pandas DataFrame for further processing
    pdf = spark_df.toPandas()

    # Duplicate detection using our pandas function
    duplicates_df = detect_duplicates(pdf)
    if duplicates_df.empty:
        duplicate_report = pd.DataFrame({"Message": ["No duplicate entries found."]})
    else:
        duplicate_report = duplicates_df.copy()

    processed_preview = pdf[['text_column', 'formatting_issues',
                             'conflicting_information', 'data_anomalies',
                             'corrected_text']]

    # Save the corrected DataFrame as a downloadable CSV file
    corrected_filename = "corrected_document.csv"
    pdf.to_csv(corrected_filename, index=False)

    # -------------------------------
    # Hadoop: Upload Corrected File to HDFS
    # -------------------------------
    hdfs_destination = "/user/your_username/corrected_document.csv"  # Adjust based on your Hadoop config
    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)

    # -------------------------------
    # Analysis & Graph Generation
    # -------------------------------
    total_records = len(pdf)
    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()
    issues_counts = {}
    for idx, row in pdf.iterrows():
        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']
        for issue in issues:
            issues_counts[issue] = issues_counts.get(issue, 0) + 1

    analysis_text = (
        f"Analysis Summary:\n"
        f"Total records: {total_records}\n"
        f"Records with at least one inconsistency: {records_with_errors}\n"
        f"Breakdown of inconsistency types:\n"
    )
    for issue, count in issues_counts.items():
        analysis_text += f" - {issue}: {count} occurrence(s)\n"
    analysis_text += f"\nHadoop Upload Status: {hadoop_status}"

    fig, ax = plt.subplots(figsize=(8, 4))
    if issues_counts:
        x_vals = list(issues_counts.keys())
        y_vals = list(issues_counts.values())
        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')
        ax.set_title("Trend of Detected Inconsistencies")
        ax.set_xlabel("Inconsistency Type")
        ax.set_ylabel("Count")
        plt.xticks(rotation=45, ha="right")
    else:
        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',
                verticalalignment='center', transform=ax.transAxes, fontsize=12)
        ax.set_axis_off()
    plt.tight_layout()

    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig

# -------------------------------
# Custom CSS for Black Background Interface
# -------------------------------

custom_css = """
body {
  background-color: #000;
  color: #fff;
}
.container {
  border: 1px solid #444;
  border-radius: 10px;
  padding: 20px;
  background-color: #222;
  margin: 10px;
}
h1, h2, h3, h4, h5, h6 {
  color: #fff;
  text-align: center;
}
"""

# -------------------------------
# Build and Launch the Gradio Blocks Interface
# -------------------------------

with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>")
    gr.Markdown(
        "Upload a file containing a column named **'text_column'**. "
        "This tool uses Apache Spark for data processing and Hadoop for file storage. "
        "It detects formatting issues, conflicting information, and data anomalies, "
        "and then produces a corrected file for download. "
        "Supported formats: CSV, TXT, JSON, XML."
    )
    with gr.Row():
        with gr.Column(elem_classes="container"):
            file_input = gr.File(label="Upload File", file_count="single", file_types=['.csv', '.txt', '.json', '.xml'])
            process_button = gr.Button("Process Document")
        with gr.Column(elem_classes="container"):
            download_csv = gr.File(label="Download Corrected CSV")
    with gr.Row():
        with gr.Column(elem_classes="container"):
            processed_preview = gr.Dataframe(label="Processed Data Preview")
        with gr.Column(elem_classes="container"):
            duplicate_report = gr.Dataframe(label="Duplicate Report")
    with gr.Row():
        with gr.Column(elem_classes="container"):
            analysis_summary = gr.Textbox(label="Analysis Summary", lines=8)
        with gr.Column(elem_classes="container"):
            plot_output = gr.Plot(label="Inconsistencies Trend Graph")
    process_button.click(
        fn=process_file,
        inputs=file_input,
        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]
    )

# Launch with debug turned off (set debug=True for additional logs)
demo.launch(share=True, debug=False)

pip install gradio pandas matplotlib pyspark

import os
import json
import pandas as pd
import re
import gradio as gr
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Hadoop Integration Function
# -------------------------------
def upload_to_hdfs(local_file, hdfs_path):
    """
    Uploads a local file to HDFS using a shell command.
    Requires Hadoop to be installed and HDFS running.
    Wrapped in try/except to avoid stopping the pipeline if HDFS isn't available.
    """
    try:
        command = f"hdfs dfs -put -f {local_file} {hdfs_path}"
        os.system(command)
        return f"File uploaded to HDFS at: {hdfs_path}"
    except Exception as e:
        return f"Hadoop upload failed: {e}"

# -------------------------------
# Gradio Interface Processing Function with Spark & Hadoop
# -------------------------------
def process_file(file_obj):
    # Check if a file is provided
    if file_obj is None:
        return pd.DataFrame(), pd.DataFrame(), None, "No file uploaded.", None

    # Extract the file path from the uploaded file object.
    # Gradio returns a dict with a "name" key.
    file_path = file_obj["name"] if isinstance(file_obj, dict) else file_obj.name
    ext = os.path.splitext(file_path)[1].lower()

    # Initialize Spark session
    spark = SparkSession.builder.master("local[*]").appName("InconsistencyDetection").getOrCreate()

    try:
        if ext == ".csv":
            spark_df = spark.read.option("header", "true").csv(file_path)
            # Check for required column in CSV file
            if "text_column" not in spark_df.columns:
                return pd.DataFrame({"Error": ["CSV file does not contain a 'text_column'."]}), pd.DataFrame(), None, "CSV missing 'text_column'.", None
        elif ext == ".txt":
            spark_df = spark.read.text(file_path)
            spark_df = spark_df.withColumnRenamed("value", "text_column")
        elif ext == ".json":
            try:
                spark_df = spark.read.json(file_path)
                if "text_column" not in spark_df.columns:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list) and all(isinstance(x, str) for x in data):
                        spark_df = spark.createDataFrame([(x,) for x in data], ["text_column"])
                    else:
                        return pd.DataFrame({"Error": ["JSON file does not contain a 'text_column'."]}), pd.DataFrame(), None, "JSON missing 'text_column'.", None
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading JSON file: {e}"]}), pd.DataFrame(), None, f"Error reading JSON: {e}", None
        elif ext == ".xml":
            try:
                df = pd.read_xml(file_path)
                if 'text_column' not in df.columns:
                    return pd.DataFrame({"Error": ["XML file does not contain a 'text_column'."]}), pd.DataFrame(), None, "XML missing 'text_column'.", None
                spark_df = spark.createDataFrame(df)
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading XML file: {e}"]}), pd.DataFrame(), None, f"Error reading XML: {e}", None
        else:
            return pd.DataFrame({"Error": ["Unsupported file format. Supported formats: CSV, TXT, JSON, XML."]}), pd.DataFrame(), None, "Unsupported file format.", None
    except Exception as e:
        return pd.DataFrame({"Error": [f"Error reading file: {e}"]}), pd.DataFrame(), None, f"Error reading file: {e}", None

    # Define Spark UDFs wrapping our Python functions
    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))
    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))
    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))
    correct_text_udf = udf(lambda x: correct_text(x), StringType())

    # Apply UDFs to create new columns
    spark_df = spark_df.withColumn("formatting_issues", detect_formatting_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("conflicting_information", detect_conflict_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("data_anomalies", detect_anomalies_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("corrected_text", correct_text_udf(spark_df["text_column"]))

    # Convert the Spark DataFrame to a Pandas DataFrame for further processing
    pdf = spark_df.toPandas()

    # Duplicate detection using our pandas function
    duplicates_df = detect_duplicates(pdf)
    if duplicates_df.empty:
        duplicate_report = pd.DataFrame({"Message": ["No duplicate entries found."]})
    else:
        duplicate_report = duplicates_df.copy()

    processed_preview = pdf[['text_column', 'formatting_issues',
                             'conflicting_information', 'data_anomalies',
                             'corrected_text']]

    # Save the corrected DataFrame as a downloadable CSV file
    corrected_filename = "corrected_document.csv"
    pdf.to_csv(corrected_filename, index=False)

    # -------------------------------
    # Hadoop: Upload Corrected File to HDFS
    # -------------------------------
    # Adjust the HDFS path based on your Hadoop configuration; if Hadoop is not configured,
    # this function will simply return an error message without halting processing.
    hdfs_destination = "/user/your_username/corrected_document.csv"
    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)

    # -------------------------------
    # Analysis & Graph Generation
    # -------------------------------
    total_records = len(pdf)
    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()

    issues_counts = {}
    for idx, row in pdf.iterrows():
        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']
        for issue in issues:
            issues_counts[issue] = issues_counts.get(issue, 0) + 1

    analysis_text = f"Analysis Summary:\nTotal records: {total_records}\n"
    analysis_text += f"Records with at least one inconsistency: {records_with_errors}\n"
    analysis_text += "Breakdown of inconsistency types:\n"
    for issue, count in issues_counts.items():
        analysis_text += f" - {issue}: {count} occurrence(s)\n"
    analysis_text += f"\nHadoop Upload Status: {hadoop_status}"

    fig, ax = plt.subplots(figsize=(8, 4))
    if issues_counts:
        x_vals = list(issues_counts.keys())
        y_vals = list(issues_counts.values())
        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')
        ax.set_title("Trend of Detected Inconsistencies")
        ax.set_xlabel("Inconsistency Type")
        ax.set_ylabel("Count")
        plt.xticks(rotation=45, ha="right")
    else:
        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',
                verticalalignment='center', transform=ax.transAxes, fontsize=12)
        ax.set_axis_off()
    plt.tight_layout()

    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig

# -------------------------------
# Custom CSS for Black Background Interface
# -------------------------------
custom_css = """
body {
  background-color: #000;
  color: #fff;
}
.container {
  border: 1px solid #444;
  border-radius: 10px;
  padding: 20px;
  background-color: #222;
  margin: 10px;
}
h1, h2, h3, h4, h5, h6 {
  color: #fff;
  text-align: center;
}
"""

# -------------------------------
# Build and Launch the Gradio Blocks Interface
# -------------------------------
with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>")
    gr.Markdown(
        "Upload a file containing a column named **'text_column'**. "
        "This tool uses Apache Spark for data processing and Hadoop for file storage. "
        "It detects formatting issues, conflicting information, and data anomalies, "
        "and then produces a corrected file for download. "
        "Supported formats: CSV, TXT, JSON, XML."
    )

    with gr.Row():
        with gr.Column(elem_classes="container"):
            file_input = gr.File(label="Upload File", file_count="single", file_types=['.csv', '.txt', '.json', '.xml'])
            process_button = gr.Button("Process Document")
        with gr.Column(elem_classes="container"):
            # This widget provides the download link for the corrected CSV file.
            download_csv = gr.File(label="Download Corrected CSV")

    with gr.Row():
        with gr.Column(elem_classes="container"):
            processed_preview = gr.Dataframe(label="Processed Data Preview")
        with gr.Column(elem_classes="container"):
            duplicate_report = gr.Dataframe(label="Duplicate Report")

    with gr.Row():
        with gr.Column(elem_classes="container"):
            analysis_summary = gr.Textbox(label="Analysis Summary", lines=8)
        with gr.Column(elem_classes="container"):
            plot_output = gr.Plot(label="Inconsistencies Trend Graph")

    process_button.click(
        fn=process_file,
        inputs=file_input,
        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]
    )

demo.launch(share=True, inline=True)

import os
import json
import time
import pandas as pd
import re
import gradio as gr
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Hadoop Integration Function
# -------------------------------
def upload_to_hdfs(local_file, hdfs_path):
    """
    Uploads a local file to HDFS using a shell command.
    Requires Hadoop to be installed and HDFS running.
    """
    command = f"hdfs dfs -put -f {local_file} {hdfs_path}"
    os.system(command)
    return f"File uploaded to HDFS at: {hdfs_path}"

# -------------------------------
# Gradio Interface Processing Function with Spark & Hadoop
# -------------------------------

def process_file(file_obj):
    if file_obj is None:
        return pd.DataFrame(), pd.DataFrame(), None, "", None

    # Start execution timer
    start_time = time.time()

    ext = os.path.splitext(file_obj.name)[1].lower()

    # Initialize Spark session
    spark = SparkSession.builder.master("local[*]").appName("InconsistencyDetection").getOrCreate()

    try:
        if ext == ".csv":
            spark_df = spark.read.option("header", "true").csv(file_obj.name)
        elif ext in [".txt"]:
            spark_df = spark.read.text(file_obj.name)
            spark_df = spark_df.withColumnRenamed("value", "text_column")
        elif ext in [".json"]:
            try:
                spark_df = spark.read.json(file_obj.name)
                if "text_column" not in spark_df.columns:
                    with open(file_obj.name, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list) and all(isinstance(x, str) for x in data):
                        spark_df = spark.createDataFrame([(x,) for x in data], ["text_column"])
                    else:
                        return pd.DataFrame({"Error": ["JSON file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading JSON file: {e}"]}), pd.DataFrame(), None, "", None
        elif ext in [".xml"]:
            try:
                df = pd.read_xml(file_obj.name)
                if 'text_column' not in df.columns:
                    return pd.DataFrame({"Error": ["XML file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
                spark_df = spark.createDataFrame(df)
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading XML file: {e}"]}), pd.DataFrame(), None, "", None
        else:
            return pd.DataFrame({"Error": ["Unsupported file format. Supported formats: CSV, TXT, JSON, XML."]}), pd.DataFrame(), None, "", None
    except Exception as e:
        return pd.DataFrame({"Error": [f"Error reading file: {e}"]}), pd.DataFrame(), None, "", None

    # Define Spark UDFs wrapping our Python functions
    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))
    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))
    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))
    correct_text_udf = udf(lambda x: correct_text(x), StringType())

    # Apply UDFs to create new columns
    spark_df = spark_df.withColumn("formatting_issues", detect_formatting_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("conflicting_information", detect_conflict_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("data_anomalies", detect_anomalies_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("corrected_text", correct_text_udf(spark_df["text_column"]))

    # Convert the Spark DataFrame to a Pandas DataFrame for further processing
    pdf = spark_df.toPandas()

    # Duplicate detection using our pandas function
    duplicates_df = detect_duplicates(pdf)
    if duplicates_df.empty:
        duplicate_report = pd.DataFrame({"Message": ["No duplicate entries found."]})
    else:
        duplicate_report = duplicates_df.copy()

    processed_preview = pdf[['text_column', 'formatting_issues',
                             'conflicting_information', 'data_anomalies',
                             'corrected_text']]

    # Save the corrected DataFrame as a downloadable CSV file
    corrected_filename = "corrected_document.csv"
    pdf.to_csv(corrected_filename, index=False)

    # -------------------------------
    # Hadoop: Upload Corrected File to HDFS
    # -------------------------------
    hdfs_destination = "/user/your_username/corrected_document.csv"
    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)

    # -------------------------------
    # Analysis & Graph Generation
    # -------------------------------
    total_records = len(pdf)
    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()

    # Calculate accuracy rate as the percentage of records without issues
    accuracy_rate = ((total_records - records_with_errors) / total_records * 100) if total_records > 0 else 0

    issues_counts = {}
    for idx, row in pdf.iterrows():
        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']
        for issue in issues:
            issues_counts[issue] = issues_counts.get(issue, 0) + 1

    # End execution timer and compute processing time
    end_time = time.time()
    execution_time = end_time - start_time

    analysis_text = f"Analysis Summary:\nTotal records: {total_records}\n"
    analysis_text += f"Records with at least one inconsistency: {records_with_errors}\n"
    analysis_text += f"Accuracy Rate: {accuracy_rate:.2f}%\n"
    analysis_text += f"Execution Time: {execution_time:.2f} seconds\n"
    analysis_text += "Breakdown of inconsistency types:\n"
    for issue, count in issues_counts.items():
        analysis_text += f" - {issue}: {count} occurrence(s)\n"
    analysis_text += f"\nHadoop Upload Status: {hadoop_status}"

    # Simulate scale-up: Projected execution times for increasing dataset sizes.
    # (These factors are for demonstration purposes.)
    scale_factors = [1, 2, 3, 4, 5]
    simulated_times = [execution_time * factor * 0.9 for factor in scale_factors]

    # Create subplots for Accuracy Rate, Execution Time, and Scale-Up Analysis
    fig, axs = plt.subplots(1, 3, figsize=(18, 5))

    # Graph 1: Accuracy Rate
    axs[0].bar(["Accuracy Rate"], [accuracy_rate], color='green')
    axs[0].set_ylim(0, 100)
    axs[0].set_title("Accuracy Rate")
    axs[0].set_ylabel("Percentage (%)")

    # Graph 2: Execution Time
    axs[1].bar(["Execution Time"], [execution_time], color='blue')
    axs[1].set_title("Execution Time")
    axs[1].set_ylabel("Time (seconds)")

    # Graph 3: Scale-Up Analysis
    axs[2].plot(scale_factors, simulated_times, marker='o', linestyle='-', color='orange')
    axs[2].set_title("Scale-Up Analysis")
    axs[2].set_xlabel("Scale Factor (Dataset Size Multiplier)")
    axs[2].set_ylabel("Simulated Execution Time (s)")
    axs[2].grid(True)

    plt.tight_layout()

    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig

# -------------------------------
# Custom CSS for Black Background Interface
# -------------------------------
custom_css = """
body {
  background-color: #000;
  color: #fff;
}
.container {
  border: 1px solid #444;
  border-radius: 10px;
  padding: 20px;
  background-color: #222;
  margin: 10px;
}
h1, h2, h3, h4, h5, h6 {
  color: #fff;
  text-align: center;
}
"""

# -------------------------------
# Build and Launch the Gradio Blocks Interface
# -------------------------------
with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>")
    gr.Markdown(
        "Upload a file containing a column named **'text_column'**. "
        "This tool uses Apache Spark for data processing and Hadoop for file storage. "
        "It detects formatting issues, conflicting information, and data anomalies, "
        "and then produces a corrected file for download. "
        "Supported formats: CSV, TXT, JSON, XML."
    )

    with gr.Row():
        with gr.Column(elem_classes="container"):
            file_input = gr.File(label="Upload File", file_count="single", file_types=['.csv', '.txt', '.json', '.xml'])
            process_button = gr.Button("Process Document")
        with gr.Column(elem_classes="container"):
            download_csv = gr.File(label="Download Corrected CSV")

    with gr.Row():
        with gr.Column(elem_classes="container"):
            processed_preview = gr.Dataframe(label="Processed Data Preview")
        with gr.Column(elem_classes="container"):
            duplicate_report = gr.Dataframe(label="Duplicate Report")

    with gr.Row():
        with gr.Column(elem_classes="container"):
            analysis_summary = gr.Textbox(label="Analysis Summary", lines=8)
        with gr.Column(elem_classes="container"):
            plot_output = gr.Plot(label="Performance Metrics Graphs")

    process_button.click(
        fn=process_file,
        inputs=file_input,
        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]
    )

demo.launch(share=True, inline=True)

"""Code to Generate Document with Inconsistencies

"""

import os
import json
import time
import pandas as pd
import re
import gradio as gr
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Hadoop Integration Function
# -------------------------------
def upload_to_hdfs(local_file, hdfs_path):
    """
    Uploads a local file to HDFS using a shell command.
    Requires Hadoop to be installed and HDFS running.
    """
    command = f"hdfs dfs -put -f {local_file} {hdfs_path}"
    os.system(command)
    return f"File uploaded to HDFS at: {hdfs_path}"

# -------------------------------
# Gradio Interface Processing Function with Spark & Hadoop
# -------------------------------

def process_file(file_obj):
    if file_obj is None:
        return pd.DataFrame(), pd.DataFrame(), None, "", None, None, None

    # Start execution timer
    start_time = time.time()

    ext = os.path.splitext(file_obj.name)[1].lower()

    # Initialize Spark session
    spark = SparkSession.builder.master("local[*]").appName("InconsistencyDetection").getOrCreate()

    try:
        if ext == ".csv":
            spark_df = spark.read.option("header", "true").csv(file_obj.name)
        elif ext in [".txt"]:
            spark_df = spark.read.text(file_obj.name)
            spark_df = spark_df.withColumnRenamed("value", "text_column")
        elif ext in [".json"]:
            try:
                spark_df = spark.read.json(file_obj.name)
                if "text_column" not in spark_df.columns:
                    with open(file_obj.name, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list) and all(isinstance(x, str) for x in data):
                        spark_df = spark.createDataFrame([(x,) for x in data], ["text_column"])
                    else:
                        return pd.DataFrame({"Error": ["JSON file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None, None, None
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading JSON file: {e}"]}), pd.DataFrame(), None, "", None, None, None
        elif ext in [".xml"]:
            try:
                df = pd.read_xml(file_obj.name)
                if 'text_column' not in df.columns:
                    return pd.DataFrame({"Error": ["XML file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None, None, None
                spark_df = spark.createDataFrame(df)
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading XML file: {e}"]}), pd.DataFrame(), None, "", None, None, None
        else:
            return pd.DataFrame({"Error": ["Unsupported file format. Supported formats: CSV, TXT, JSON, XML."]}), pd.DataFrame(), None, "", None, None, None
    except Exception as e:
        return pd.DataFrame({"Error": [f"Error reading file: {e}"]}), pd.DataFrame(), None, "", None, None, None

    # Define Spark UDFs wrapping our Python functions
    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))
    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))
    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))
    correct_text_udf = udf(lambda x: correct_text(x), StringType())

    # Apply UDFs to create new columns
    spark_df = spark_df.withColumn("formatting_issues", detect_formatting_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("conflicting_information", detect_conflict_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("data_anomalies", detect_anomalies_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("corrected_text", correct_text_udf(spark_df["text_column"]))

    # Convert the Spark DataFrame to a Pandas DataFrame for further processing
    pdf = spark_df.toPandas()

    # Duplicate detection using our pandas function
    duplicates_df = detect_duplicates(pdf)
    if duplicates_df.empty:
        duplicate_report = pd.DataFrame({"Message": ["No duplicate entries found."]})
    else:
        duplicate_report = duplicates_df.copy()

    processed_preview = pdf[['text_column', 'formatting_issues',
                             'conflicting_information', 'data_anomalies',
                             'corrected_text']]

    # Save the corrected DataFrame as a downloadable CSV file
    corrected_filename = "corrected_document.csv"
    pdf.to_csv(corrected_filename, index=False)

    # -------------------------------
    # Hadoop: Upload Corrected File to HDFS
    # -------------------------------
    hdfs_destination = "/user/your_username/corrected_document.csv"
    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)

    # -------------------------------
    # Analysis & Graph Generation
    # -------------------------------
    total_records = len(pdf)
    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()

    # Calculate accuracy rate as the percentage of records without issues
    accuracy_rate = ((total_records - records_with_errors) / total_records * 100) if total_records > 0 else 0

    issues_counts = {}
    for idx, row in pdf.iterrows():
        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']
        for issue in issues:
            issues_counts[issue] = issues_counts.get(issue, 0) + 1

    # End execution timer and compute processing time
    end_time = time.time()
    execution_time = end_time - start_time

    analysis_text = f"Analysis Summary:\nTotal records: {total_records}\n"
    analysis_text += f"Records with at least one inconsistency: {records_with_errors}\n"
    analysis_text += f"Accuracy Rate: {accuracy_rate:.2f}%\n"
    analysis_text += f"Execution Time: {execution_time:.2f} seconds\n"
    analysis_text += "Breakdown of inconsistency types:\n"
    for issue, count in issues_counts.items():
        analysis_text += f" - {issue}: {count} occurrence(s)\n"
    analysis_text += f"\nHadoop Upload Status: {hadoop_status}"

    # Simulate scale-up: Projected execution times for increasing dataset sizes.
    # (These factors are for demonstration purposes.)
    scale_factors = [1, 2, 3, 4, 5]
    simulated_times = [execution_time * factor * 0.9 for factor in scale_factors]

    # -------------------------------
    # Create Individual Graphs
    # -------------------------------

    # Graph 1: Accuracy Rate
    fig_accuracy, ax1 = plt.subplots(figsize=(6, 4))
    ax1.bar(["Accuracy Rate"], [accuracy_rate], color='green')
    ax1.set_ylim(0, 100)
    ax1.set_title("Accuracy Rate")
    ax1.set_ylabel("Percentage (%)")
    plt.tight_layout()

    # Graph 2: Execution Time
    fig_execution, ax2 = plt.subplots(figsize=(6, 4))
    ax2.bar(["Execution Time"], [execution_time], color='blue')
    ax2.set_title("Execution Time")
    ax2.set_ylabel("Time (seconds)")
    plt.tight_layout()

    # Graph 3: Scale-Up Analysis
    fig_scaleup, ax3 = plt.subplots(figsize=(6, 4))
    ax3.plot(scale_factors, simulated_times, marker='o', linestyle='-', color='orange')
    ax3.set_title("Scale-Up Analysis")
    ax3.set_xlabel("Scale Factor (Dataset Size Multiplier)")
    ax3.set_ylabel("Simulated Execution Time (s)")
    ax3.grid(True)
    plt.tight_layout()

    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig_accuracy, fig_execution, fig_scaleup

# -------------------------------
# Custom CSS for Black Background Interface
# -------------------------------
custom_css = """
body {
  background-color: #000;
  color: #fff;
}
.container {
  border: 1px solid #444;
  border-radius: 10px;
  padding: 20px;
  background-color: #222;
  margin: 10px;
}
h1, h2, h3, h4, h5, h6 {
  color: #fff;
  text-align: center;
}
"""

# -------------------------------
# Build and Launch the Gradio Blocks Interface
# -------------------------------
with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>")
    gr.Markdown(
        "Upload a file containing a column named **'text_column'**. "
        "This tool uses Apache Spark for data processing and Hadoop for file storage. "
        "It detects formatting issues, conflicting information, and data anomalies, "
        "and then produces a corrected file for download. "
        "Supported formats: CSV, TXT, JSON, XML."
    )

    with gr.Row():
        with gr.Column(elem_classes="container"):
            file_input = gr.File(label="Upload File", file_count="single", file_types=['.csv', '.txt', '.json', '.xml'])
            process_button = gr.Button("Process Document")
        with gr.Column(elem_classes="container"):
            download_csv = gr.File(label="Download Corrected CSV")

    with gr.Row():
        with gr.Column(elem_classes="container"):
            processed_preview = gr.Dataframe(label="Processed Data Preview")
        with gr.Column(elem_classes="container"):
            duplicate_report = gr.Dataframe(label="Duplicate Report")

    with gr.Row():
        with gr.Column(elem_classes="container"):
            analysis_summary = gr.Textbox(label="Analysis Summary", lines=8)

    with gr.Row():
        with gr.Column(elem_classes="container"):
            plot_accuracy = gr.Plot(label="Accuracy Rate")
        with gr.Column(elem_classes="container"):
            plot_execution = gr.Plot(label="Execution Time")
        with gr.Column(elem_classes="container"):
            plot_scaleup = gr.Plot(label="Scale-Up Analysis")

    process_button.click(
        fn=process_file,
        inputs=file_input,
        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary,
                 plot_accuracy, plot_execution, plot_scaleup]
    )

demo.launch(share=True, inline=True)

"""generate document with inconsistencies upto 100 lines"""

import pandas as pd
import random

# Define categories of inconsistencies
conflict_pairs = [
    ("increase", "decrease"), ("up", "down"), ("true", "false"),
    ("positive", "negative"), ("win", "loss"), ("hot", "cold")
]

formatting_issues = [
    "this sentence lacks capitalization.",  # No capitalization
    "Missing punctuation at the end",  # No punctuation
    "Excessive punctuation!!!",  # Too much punctuation
    "Short"  # Too short
]

duplicates = [
    "Repeated text entry for testing purposes.",
    "Duplicated line appearing multiple times in dataset."
]

# Generate 100 rows of inconsistent data
data = []
for i in range(1, 101):
    inconsistency_type = random.choice(["format", "conflict", "duplicate"])

    if inconsistency_type == "format":
        text = random.choice(formatting_issues)
    elif inconsistency_type == "conflict":
        pair = random.choice(conflict_pairs)
        text = f"{pair[0]} and {pair[1]} appear together."
    else:  # duplicate
        text = random.choice(duplicates)

    data.append({"id": i, "text_column": text})

# Create DataFrame
df_inconsistent = pd.DataFrame(data)

# Save to CSV
csv_filename = "inconsistent_sample_100.csv"
df_inconsistent.to_csv(csv_filename, index=False)

# Display sample rows
print("Generated dataset with inconsistencies:")
print(df_inconsistent.head(10))

import os
import json
import pandas as pd
import re
import gradio as gr
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType
import subprocess

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Hadoop Integration Function
# -------------------------------

def upload_to_hdfs(local_file, hdfs_path):
    """
    Uploads a local file to HDFS using a shell command.
    Requires Hadoop to be installed and HDFS running.
    """
    try:
        result = subprocess.run(
            ["hdfs", "dfs", "-put", "-f", local_file, hdfs_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        return f"File uploaded to HDFS at: {hdfs_path}"
    except subprocess.CalledProcessError as e:
        return f"Failed to upload to HDFS: {e.stderr.decode('utf-8')}"

# -------------------------------
# Gradio Interface Processing Function with Spark & Hadoop
# -------------------------------

def process_file(file_obj):
    if file_obj is None:
        return pd.DataFrame(), pd.DataFrame(), None, "", None

    # Handle file object: if it's a dict, use the 'name' key.
    file_path = file_obj.name if hasattr(file_obj, "name") else file_obj["name"]
    ext = os.path.splitext(file_path)[1].lower()

    # Initialize Spark session
    spark = SparkSession.builder.master("local[*]").appName("InconsistencyDetection").getOrCreate()

    try:
        if ext == ".csv":
            spark_df = spark.read.option("header", "true").csv(file_path)
        elif ext == ".txt":
            spark_df = spark.read.text(file_path)
            spark_df = spark_df.withColumnRenamed("value", "text_column")
        elif ext == ".json":
            try:
                spark_df = spark.read.json(file_path)
                if "text_column" not in spark_df.columns:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list) and all(isinstance(x, str) for x in data):
                        spark_df = spark.createDataFrame([(x,) for x in data], ["text_column"])
                    else:
                        return pd.DataFrame({"Error": ["JSON file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading JSON file: {e}"]}), pd.DataFrame(), None, "", None
        elif ext == ".xml":
            try:
                df = pd.read_xml(file_path)
                if 'text_column' not in df.columns:
                    return pd.DataFrame({"Error": ["XML file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
                spark_df = spark.createDataFrame(df)
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading XML file: {e}"]}), pd.DataFrame(), None, "", None
        else:
            return pd.DataFrame({"Error": ["Unsupported file format. Supported formats: CSV, TXT, JSON, XML."]}), pd.DataFrame(), None, "", None
    except Exception as e:
        return pd.DataFrame({"Error": [f"Error reading file: {e}"]}), pd.DataFrame(), None, "", None

    # Define Spark UDFs wrapping our Python functions
    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))
    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))
    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))
    correct_text_udf = udf(lambda x: correct_text(x), StringType())

    # Apply UDFs to create new columns
    spark_df = spark_df.withColumn("formatting_issues", detect_formatting_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("conflicting_information", detect_conflict_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("data_anomalies", detect_anomalies_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("corrected_text", correct_text_udf(spark_df["text_column"]))

    # Convert the Spark DataFrame to a Pandas DataFrame for further processing
    pdf = spark_df.toPandas()

    # Duplicate detection using our pandas function
    duplicates_df = detect_duplicates(pdf)
    if duplicates_df.empty:
        duplicate_report = pd.DataFrame({"Message": ["No duplicate entries found."]})
    else:
        duplicate_report = duplicates_df.copy()

    processed_preview = pdf[['text_column', 'formatting_issues',
                             'conflicting_information', 'data_anomalies',
                             'corrected_text']]

    # Save the corrected DataFrame as a downloadable CSV file
    corrected_filename = "corrected_document.csv"
    pdf.to_csv(corrected_filename, index=False)

    # -------------------------------
    # Hadoop: Upload Corrected File to HDFS
    # -------------------------------
    hdfs_destination = "/user/your_username/corrected_document.csv"  # Adjust based on your Hadoop config
    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)

    # -------------------------------
    # Analysis & Graph Generation
    # -------------------------------
    total_records = len(pdf)
    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()
    issues_counts = {}
    for idx, row in pdf.iterrows():
        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']
        for issue in issues:
            issues_counts[issue] = issues_counts.get(issue, 0) + 1

    analysis_text = (
        f"Analysis Summary:\n"
        f"Total records: {total_records}\n"
        f"Records with at least one inconsistency: {records_with_errors}\n"
        f"Breakdown of inconsistency types:\n"
    )
    for issue, count in issues_counts.items():
        analysis_text += f" - {issue}: {count} occurrence(s)\n"
    analysis_text += f"\nHadoop Upload Status: {hadoop_status}"

    fig, ax = plt.subplots(figsize=(8, 4))
    if issues_counts:
        x_vals = list(issues_counts.keys())
        y_vals = list(issues_counts.values())
        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')
        ax.set_title("Trend of Detected Inconsistencies")
        ax.set_xlabel("Inconsistency Type")
        ax.set_ylabel("Count")
        plt.xticks(rotation=45, ha="right")
    else:
        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',
                verticalalignment='center', transform=ax.transAxes, fontsize=12)
        ax.set_axis_off()
    plt.tight_layout()

    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig

# -------------------------------
# Custom CSS for Black Background Interface
# -------------------------------

custom_css = """
body {
  background-color: #000;
  color: #fff;
}
.container {
  border: 1px solid #444;
  border-radius: 10px;
  padding: 20px;
  background-color: #222;
  margin: 10px;
}
h1, h2, h3, h4, h5, h6 {
  color: #fff;
  text-align: center;
}
"""

# -------------------------------
# Build and Launch the Gradio Blocks Interface
# -------------------------------

with gr.Blocks(css=custom_css) as demo:
    gr.Markdown("<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>")
    gr.Markdown(
        "Upload a file containing a column named **'text_column'**. "
        "This tool uses Apache Spark for data processing and Hadoop for file storage. "
        "It detects formatting issues, conflicting information, and data anomalies, "
        "and then produces a corrected file for download. "
        "Supported formats: CSV, TXT, JSON, XML."
    )
    with gr.Row():
        with gr.Column(elem_classes="container"):
            file_input = gr.File(label="Upload File", file_count="single", file_types=['.csv', '.txt', '.json', '.xml'])
            process_button = gr.Button("Process Document")
        with gr.Column(elem_classes="container"):
            download_csv = gr.File(label="Download Corrected CSV")
    with gr.Row():
        with gr.Column(elem_classes="container"):
            processed_preview = gr.Dataframe(label="Processed Data Preview")
        with gr.Column(elem_classes="container"):
            duplicate_report = gr.Dataframe(label="Duplicate Report")
    with gr.Row():
        with gr.Column(elem_classes="container"):
            analysis_summary = gr.Textbox(label="Analysis Summary", lines=8)
        with gr.Column(elem_classes="container"):
            plot_output = gr.Plot(label="Inconsistencies Trend Graph")
    process_button.click(
        fn=process_file,
        inputs=file_input,
        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]
    )

# Launch with debug=True for Colab to display errors and logs
demo.launch(share=True, debug=True)

import os
import json
import pandas as pd
import re
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession
from pyspark.sql.types import ArrayType, StringType
from pyspark.sql.functions import pandas_udf
import subprocess
import ipywidgets as widgets
from IPython.display import display, HTML, FileLink, clear_output

# -------------------------------
# Inconsistency Detection Functions
# -------------------------------

def detect_formatting_issues(text):
    """
    Checks whether the text:
      - Starts with an uppercase letter.
      - Ends with proper punctuation (., !, or ?).
    Returns a list of formatting issues.
    """
    issues = []
    if not text:
        issues.append("Empty text")
        return issues
    if not text[0].isupper():
        issues.append("Does not start with an uppercase letter")
    if text[-1] not in ['.', '!', '?']:
        issues.append("Does not end with proper punctuation")
    return issues

def detect_conflicting_information(text):
    """
    Checks for pairs of contradictory keywords in the text.
    Returns a list of detected conflicts.
    """
    conflict_pairs = [
        ('increase', 'decrease'),
        ('up', 'down'),
        ('true', 'false'),
        ('positive', 'negative'),
        ('win', 'loss'),
        ('hot', 'cold')
    ]
    conflicts = []
    text_lower = text.lower()
    for w1, w2 in conflict_pairs:
        if w1 in text_lower and w2 in text_lower:
            conflicts.append(f"Conflict: {w1} vs {w2}")
    return conflicts

def detect_data_anomalies(text):
    """
    Detects data anomalies such as:
      - Empty text.
      - Text that is extremely short.
      - Excessive punctuation (three or more punctuation marks in a row).
      - A high ratio of non-alphanumeric to alphanumeric characters.
    Returns a list of anomaly issues.
    """
    anomalies = []
    if not text or len(text.strip()) == 0:
        anomalies.append("Empty text anomaly")
    if len(text) < 5:
        anomalies.append("Text too short")
    if re.search(r'[\!\?\.]{3,}', text):
        anomalies.append("Excessive punctuation")
    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)
    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\s]', text)
    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:
        anomalies.append("High non-alphanumeric ratio")
    return anomalies

def detect_duplicates(df, text_column='text_column'):
    """
    Returns a DataFrame with rows that have duplicate values in the specified text column.
    """
    duplicates = df[df.duplicated(subset=[text_column], keep=False)]
    return duplicates

def correct_text(text):
    """
    Corrects formatting issues:
      - Capitalizes the first character if necessary.
      - Ensures the text ends with a punctuation mark (adds a period if missing).
    Returns the corrected text.
    """
    if not text:
        return text
    if not text[0].isupper():
        text = text[0].upper() + text[1:]
    if text[-1] not in ['.', '!', '?']:
        text = text + '.'
    return text

# -------------------------------
# Hadoop Integration Function
# -------------------------------

def upload_to_hdfs(local_file, hdfs_path):
    """
    Uploads a local file to HDFS using a shell command.
    Requires Hadoop to be installed and HDFS running.
    """
    try:
        result = subprocess.run(
            ["hdfs", "dfs", "-put", "-f", local_file, hdfs_path],
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        return f"File uploaded to HDFS at: {hdfs_path}"
    except subprocess.CalledProcessError as e:
        return f"Failed to upload to HDFS: {e.stderr.decode('utf-8')}"

# -------------------------------
# Define Pandas UDFs for Spark Processing
# -------------------------------

@pandas_udf(ArrayType(StringType()))
def detect_formatting_udf(s: pd.Series) -> pd.Series:
    return s.apply(lambda x: detect_formatting_issues(x))

@pandas_udf(ArrayType(StringType()))
def detect_conflict_udf(s: pd.Series) -> pd.Series:
    return s.apply(lambda x: detect_conflicting_information(x))

@pandas_udf(ArrayType(StringType()))
def detect_anomalies_udf(s: pd.Series) -> pd.Series:
    return s.apply(lambda x: detect_data_anomalies(x))

@pandas_udf(StringType())
def correct_text_udf(s: pd.Series) -> pd.Series:
    return s.apply(lambda x: correct_text(x))

# -------------------------------
# ipywidgets Processing Function with Spark & Hadoop
# -------------------------------

def process_file(file_obj):
    if file_obj is None:
        return pd.DataFrame(), pd.DataFrame(), None, "", None

    # Handle file object: if it's a dict, use the 'name' key.
    file_path = file_obj.name if hasattr(file_obj, "name") else file_obj["name"]
    ext = os.path.splitext(file_path)[1].lower()

    # Initialize Spark session
    spark = SparkSession.builder.master("local[*]").appName("InconsistencyDetection").getOrCreate()

    try:
        if ext == ".csv":
            spark_df = spark.read.option("header", "true").csv(file_path)
        elif ext == ".txt":
            spark_df = spark.read.text(file_path)
            spark_df = spark_df.withColumnRenamed("value", "text_column")
        elif ext == ".json":
            try:
                spark_df = spark.read.json(file_path)
                if "text_column" not in spark_df.columns:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    if isinstance(data, list) and all(isinstance(x, str) for x in data):
                        spark_df = spark.createDataFrame([(x,) for x in data], ["text_column"])
                    else:
                        return pd.DataFrame({"Error": ["JSON file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading JSON file: {e}"]}), pd.DataFrame(), None, "", None
        elif ext == ".xml":
            try:
                df = pd.read_xml(file_path)
                if 'text_column' not in df.columns:
                    return pd.DataFrame({"Error": ["XML file does not contain a 'text_column'."]}), pd.DataFrame(), None, "", None
                spark_df = spark.createDataFrame(df)
            except Exception as e:
                return pd.DataFrame({"Error": [f"Error reading XML file: {e}"]}), pd.DataFrame(), None, "", None
        else:
            return pd.DataFrame({"Error": ["Unsupported file format. Supported formats: CSV, TXT, JSON, XML."]}), pd.DataFrame(), None, "", None
    except Exception as e:
        return pd.DataFrame({"Error": [f"Error reading file: {e}"]}), pd.DataFrame(), None, "", None

    # Apply the Pandas UDFs to create new columns
    spark_df = spark_df.withColumn("formatting_issues", detect_formatting_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("conflicting_information", detect_conflict_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("data_anomalies", detect_anomalies_udf(spark_df["text_column"]))
    spark_df = spark_df.withColumn("corrected_text", correct_text_udf(spark_df["text_column"]))

    # Convert the Spark DataFrame to a Pandas DataFrame for further processing
    pdf = spark_df.toPandas()

    # Duplicate detection using our pandas function
    duplicates_df = detect_duplicates(pdf)
    if duplicates_df.empty:
        duplicate_report = pd.DataFrame({"Message": ["No duplicate entries found."]})
    else:
        duplicate_report = duplicates_df.copy()

    processed_preview = pdf[['text_column', 'formatting_issues',
                             'conflicting_information', 'data_anomalies',
                             'corrected_text']]

    # Save the corrected DataFrame as a downloadable CSV file
    corrected_filename = "corrected_document.csv"
    pdf.to_csv(corrected_filename, index=False)

    # -------------------------------
    # Hadoop: Upload Corrected File to HDFS
    # -------------------------------
    hdfs_destination = "/user/your_username/corrected_document.csv"  # Adjust based on your Hadoop config
    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)

    # -------------------------------
    # Analysis & Graph Generation
    # -------------------------------
    total_records = len(pdf)
    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()
    issues_counts = {}
    for idx, row in pdf.iterrows():
        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']
        for issue in issues:
            issues_counts[issue] = issues_counts.get(issue, 0) + 1

    analysis_text = (
        f"Analysis Summary:\n"
        f"Total records: {total_records}\n"
        f"Records with at least one inconsistency: {records_with_errors}\n"
        f"Breakdown of inconsistency types:\n"
    )
    for issue, count in issues_counts.items():
        analysis_text += f" - {issue}: {count} occurrence(s)\n"
    analysis_text += f"\nHadoop Upload Status: {hadoop_status}"

    fig, ax = plt.subplots(figsize=(8, 4))
    if issues_counts:
        x_vals = list(issues_counts.keys())
        y_vals = list(issues_counts.values())
        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')
        ax.set_title("Trend of Detected Inconsistencies")
        ax.set_xlabel("Inconsistency Type")
        ax.set_ylabel("Count")
        plt.xticks(rotation=45, ha="right")
    else:
        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',
                verticalalignment='center', transform=ax.transAxes, fontsize=12)
        ax.set_axis_off()
    plt.tight_layout()

    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig

# -------------------------------
# ipywidgets User Interface Setup
# -------------------------------

# Inject custom CSS into the notebook for a dark theme look.
display(HTML("""
<style>
body {
  background-color: #000;
  color: #fff;
}
.widget-box {
  border: 1px solid #444;
  border-radius: 10px;
  padding: 20px;
  background-color: #222;
  margin: 10px;
}
h1, h2, h3, h4, h5, h6 {
  color: #fff;
  text-align: center;
}
</style>
"""))

# Create the file upload widget (accepting CSV, TXT, JSON, XML)
upload_widget = widgets.FileUpload(accept=".csv,.txt,.json,.xml", multiple=False, description="Upload File")

# Create the process button widget
process_button = widgets.Button(description="Process Document", button_style='info')

# Create output areas for the various outputs
download_link = widgets.HTML()  # For displaying download link for corrected CSV
processed_preview_output = widgets.Output()
duplicate_report_output = widgets.Output()
analysis_summary_output = widgets.Output()
plot_output = widgets.Output()

# Layout the UI using VBox and HBox containers
header = widgets.HTML("<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>")
description = widgets.HTML(
    "<p>Upload a file containing a column named <b>'text_column'</b>. "
    "This tool uses Apache Spark for data processing and Hadoop for file storage. "
    "It detects formatting issues, conflicting information, and data anomalies, "
    "and then produces a corrected file for download. "
    "Supported formats: CSV, TXT, JSON, XML.</p>"
)
file_controls = widgets.HBox([upload_widget, process_button])
data_outputs = widgets.HBox([processed_preview_output, duplicate_report_output])
result_outputs = widgets.HBox([analysis_summary_output, plot_output])
ui = widgets.VBox([header, description, file_controls, download_link, data_outputs, result_outputs])

# Define the callback for the process button click event
def on_process_button_clicked(b):
    # Clear previous outputs
    with processed_preview_output:
        clear_output()
    with duplicate_report_output:
        clear_output()
    with analysis_summary_output:
        clear_output()
    with plot_output:
        clear_output()
    download_link.value = ""

    # Check if a file has been uploaded
    if not upload_widget.value:
        with analysis_summary_output:
            print("Please upload a file first.")
        return

    # Retrieve the uploaded file (ipywidgets returns a dict of files)
    file_info = list(upload_widget.value.values())[0]
    # Get the file name from metadata; fallback to key name if necessary
    file_name = file_info['metadata']['name'] if 'metadata' in file_info and 'name' in file_info['metadata'] else file_info['name']

    # Write the uploaded file content to disk
    with open(file_name, 'wb') as f:
        f.write(file_info['content'])

    # Create a file object (dictionary with key 'name') as expected by process_file
    file_obj = {"name": file_name}

    # Process the file and get the outputs
    processed_preview, duplicate_report, corrected_filename, analysis_text, fig = process_file(file_obj)

    # Display the processed preview DataFrame
    with processed_preview_output:
        print("Processed Data Preview:")
        display(processed_preview)

    # Display the duplicate report DataFrame
    with duplicate_report_output:
        print("Duplicate Report:")
        display(duplicate_report)

    # Display the analysis summary text
    with analysis_summary_output:
        print(analysis_text)

    # Display the matplotlib figure
    with plot_output:
        display(fig)

    # Provide a download link for the corrected CSV file using FileLink
    download_link.value = f'<a href="{corrected_filename}" target="_blank">Download Corrected CSV</a>'

# Link the button click to the callback function
process_button.on_click(on_process_button_clicked)

# Display the entire UI
display(ui)