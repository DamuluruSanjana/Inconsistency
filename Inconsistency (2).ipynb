{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpz-UrYXOOqO",
        "outputId": "6d994324-5359-4d7a-dbf6-80daa83d2163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,677 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,762 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Fetched 24.3 MB in 3s (7,383 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common librsvg2-common libxkbfile1\n",
            "  libxt-dev libxtst6 libxxf86dga1 openjdk-8-jdk-headless openjdk-8-jre openjdk-8-jre-headless\n",
            "  x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libxt-doc openjdk-8-demo openjdk-8-source visualvm libnss-mdns fonts-nanum\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java libatk-wrapper-java-jni libfontenc1\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common librsvg2-common libxkbfile1\n",
            "  libxt-dev libxtst6 libxxf86dga1 openjdk-8-jdk openjdk-8-jdk-headless openjdk-8-jre\n",
            "  openjdk-8-jre-headless x11-utils\n",
            "0 upgraded, 20 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 50.1 MB of archives.\n",
            "After this operation, 169 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre-headless amd64 8u442-b06~us1-0ubuntu1~22.04 [30.8 MB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jre amd64 8u442-b06~us1-0ubuntu1~22.04 [75.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk-headless amd64 8u442-b06~us1-0ubuntu1~22.04 [8,864 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-8-jdk amd64 8u442-b06~us1-0ubuntu1~22.04 [4,077 kB]\n",
            "Fetched 50.1 MB in 3s (18.9 MB/s)\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../01-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../03-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../04-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../06-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../07-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../08-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../09-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../10-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../11-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../12-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../13-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../14-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../15-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../16-openjdk-8-jre-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../17-openjdk-8-jre_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../18-openjdk-8-jdk-headless_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../19-openjdk-8-jdk_8u442-b06~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u442-b06~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.3) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW-XZSLeOh4x",
        "outputId": "1aab7fdc-52e1-42e5-9246-9d37c2641cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-21 06:10:09--  https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M  5.14MB/s    in 30s     \n",
            "\n",
            "2025-03-21 06:10:40 (7.32 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnK5lHqnM0iS",
        "outputId": "eafcd6c6-dfc4-4b5a-b30f-8bbac7251f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.1 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas gradio pyspark matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX4wVSAvM6c_",
        "outputId": "f8776e68-f1c0-41f3-ffe3-d5faf282c827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.22.0)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.11)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract Spark\n",
        "!ls -l\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "# Step 3: Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "# Step 4: Initialize findspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "print(\"SPARK_HOME is set to:\", os.environ[\"SPARK_HOME\"])\n",
        "\n",
        "# Step 5: Verify Spark Installation\n",
        "!pip install -q pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"TestSpark\").getOrCreate()\n",
        "print(\"Spark session created successfully!\")\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "NAEavClBPc5I",
        "outputId": "1fdf50a1-19dc-4713-ce39-978c385298f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 223480\n",
            "drwxr-xr-x 1 root root      4096 Mar 19 13:34 sample_data\n",
            "-rw-r--r-- 1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "total 223484\n",
            "drwxr-xr-x  1 root root      4096 Mar 19 13:34 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24  2021 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "SPARK_HOME is set to: /content/spark-3.1.2-bin-hadoop3.2\n",
            "Spark session created successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f8c730835d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9df85fe4b9be:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>TestSpark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "import subprocess\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "# Set this flag to True if Hadoop is available; otherwise, leave it False.\n",
        "ENABLE_HADOOP = False\n",
        "\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"hdfs\", \"dfs\", \"-put\", \"-f\", local_file, hdfs_path],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        return f\"Failed to upload to HDFS: {e.stderr.decode('utf-8')}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Optional Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Get file path from the uploaded file object\n",
        "    file_path = file_obj.name if hasattr(file_obj, \"name\") else file_obj[\"name\"]\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "        elif ext == \".txt\":\n",
        "            spark_df = spark.read.text(file_path)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext == \".json\":\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_path)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        elif ext == \".xml\":\n",
        "            try:\n",
        "                df = pd.read_xml(file_path)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS (optional)\n",
        "    # -------------------------------\n",
        "    if ENABLE_HADOOP:\n",
        "        hdfs_destination = \"/user/your_username/corrected_document.csv\"  # Adjust based on your Hadoop config\n",
        "        hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "    else:\n",
        "        hadoop_status = \"Hadoop integration disabled.\"\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    analysis_text = (\n",
        "        f\"Analysis Summary:\\n\"\n",
        "        f\"Total records: {total_records}\\n\"\n",
        "        f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "        f\"Breakdown of inconsistency types:\\n\"\n",
        "    )\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    if issues_counts:\n",
        "        x_vals = list(issues_counts.keys())\n",
        "        y_vals = list(issues_counts.values())\n",
        "        ax.plot(x_vals, y_vals, marker='o', linestyle='-')\n",
        "        ax.set_title(\"Trend of Detected Inconsistencies\")\n",
        "        ax.set_xlabel(\"Inconsistency Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',\n",
        "                verticalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Optional Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and (optionally) Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_output = gr.Plot(label=\"Inconsistencies Trend Graph\")\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]\n",
        "    )\n",
        "\n",
        "# Launch with debug=True for Colab to display errors and logs.\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TyJjFu1bNOBx",
        "outputId": "70312f14-16c9-423f-88d1-6ed70fc453ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a7806c9e02017c9818.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a7806c9e02017c9818.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-73e0b26ad9c6>\", line 190, in process_file\n",
            "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-10-528f2d8a8060>\", line 109, in upload_to_hdfs\n",
            "    result = subprocess.run(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 548, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1026, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1955, in _execute_child\n",
            "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'hdfs'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    command = f\"hdfs dfs -put -f {local_file} {hdfs_path}\"\n",
        "    os.system(command)\n",
        "    return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None, None, None\n",
        "\n",
        "    # Start execution timer\n",
        "    start_time = time.time()\n",
        "\n",
        "    ext = os.path.splitext(file_obj.name)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_obj.name)\n",
        "        elif ext in [\".txt\"]:\n",
        "            spark_df = spark.read.text(file_obj.name)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext in [\".json\"]:\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_obj.name)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_obj.name, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "        elif ext in [\".xml\"]:\n",
        "            try:\n",
        "                df = pd.read_xml(file_obj.name)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None, None, None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "\n",
        "    # Calculate accuracy rate as the percentage of records without issues\n",
        "    accuracy_rate = ((total_records - records_with_errors) / total_records * 100) if total_records > 0 else 0\n",
        "\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    # End execution timer and compute processing time\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "\n",
        "    analysis_text = f\"Analysis Summary:\\nTotal records: {total_records}\\n\"\n",
        "    analysis_text += f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "    analysis_text += f\"Accuracy Rate: {accuracy_rate:.2f}%\\n\"\n",
        "    analysis_text += f\"Execution Time: {execution_time:.2f} seconds\\n\"\n",
        "    analysis_text += \"Breakdown of inconsistency types:\\n\"\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    # Simulate scale-up: Projected execution times for increasing dataset sizes.\n",
        "    # (These factors are for demonstration purposes.)\n",
        "    scale_factors = [1, 2, 3, 4, 5]\n",
        "    simulated_times = [execution_time * factor * 0.9 for factor in scale_factors]\n",
        "\n",
        "    # -------------------------------\n",
        "    # Create Individual Graphs\n",
        "    # -------------------------------\n",
        "\n",
        "    # Graph 1: Accuracy Rate\n",
        "    fig_accuracy, ax1 = plt.subplots(figsize=(6, 4))\n",
        "    ax1.bar([\"Accuracy Rate\"], [accuracy_rate], color='green')\n",
        "    ax1.set_ylim(0, 100)\n",
        "    ax1.set_title(\"Accuracy Rate\")\n",
        "    ax1.set_ylabel(\"Percentage (%)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Graph 2: Execution Time\n",
        "    fig_execution, ax2 = plt.subplots(figsize=(6, 4))\n",
        "    ax2.bar([\"Execution Time\"], [execution_time], color='blue')\n",
        "    ax2.set_title(\"Execution Time\")\n",
        "    ax2.set_ylabel(\"Time (seconds)\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Graph 3: Scale-Up Analysis\n",
        "    fig_scaleup, ax3 = plt.subplots(figsize=(6, 4))\n",
        "    ax3.plot(scale_factors, simulated_times, marker='o', linestyle='-', color='orange')\n",
        "    ax3.set_title(\"Scale-Up Analysis\")\n",
        "    ax3.set_xlabel(\"Scale Factor (Dataset Size Multiplier)\")\n",
        "    ax3.set_ylabel(\"Simulated Execution Time (s)\")\n",
        "    ax3.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig_accuracy, fig_execution, fig_scaleup\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_accuracy = gr.Plot(label=\"Accuracy Rate\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_execution = gr.Plot(label=\"Execution Time\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_scaleup = gr.Plot(label=\"Scale-Up Analysis\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary,\n",
        "                 plot_accuracy, plot_execution, plot_scaleup]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, inline=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "kGnS39SqX_CA",
        "outputId": "342fb83b-e4c8-4503-aa27-6bd082b4dcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0659dfbd1a4e19235d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0659dfbd1a4e19235d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}