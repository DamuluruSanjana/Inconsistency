{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3446d2c716f743f0a40e39e9518e8643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_045117c6ae9c465495cce7728d73beb5",
              "IPY_MODEL_b5e884075dfc4b97abb7c2c164df936f",
              "IPY_MODEL_2e8c65c3bbfe405aad31f2d15961d05c"
            ],
            "layout": "IPY_MODEL_cd52c45010664579a6e8c6e3b6aceffe"
          }
        },
        "045117c6ae9c465495cce7728d73beb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".csv",
            "button_style": "",
            "data": [],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_1143c6d61d51498f8f4361b3fec76737",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_1f3b051bb96d4d63808b0b434cdfdc31"
          }
        },
        "b5e884075dfc4b97abb7c2c164df936f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Process Document",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_82eef6bcfb5048e599923fa492e50f14",
            "style": "IPY_MODEL_cbf0f35359574cc08c306b2c0a1922e4",
            "tooltip": ""
          }
        },
        "2e8c65c3bbfe405aad31f2d15961d05c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_90622235dde94d75895e107f664e48ae",
            "msg_id": "",
            "outputs": []
          }
        },
        "cd52c45010664579a6e8c6e3b6aceffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1143c6d61d51498f8f4361b3fec76737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3b051bb96d4d63808b0b434cdfdc31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "82eef6bcfb5048e599923fa492e50f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbf0f35359574cc08c306b2c0a1922e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "90622235dde94d75895e107f664e48ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamuluruSanjana/Inconsistency/blob/main/INCONSISTENCY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONNECTION\n"
      ],
      "metadata": {
        "id": "89WwIEkw1gzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "# Step 2: Extract Spark\n",
        "!ls -l\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!ls -l\n",
        "\n",
        "# Step 3: Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "# Step 4: Initialize findspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "print(\"SPARK_HOME is set to:\", os.environ[\"SPARK_HOME\"])\n",
        "\n",
        "# Step 5: Verify Spark Installation\n",
        "!pip install -q pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"TestSpark\").getOrCreate()\n",
        "print(\"Spark session created successfully!\")\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "qcIFYXGGsNB4",
        "outputId": "df4849d2-84ce-4786-d71d-e03cc29cf07f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-19 16:55:43--  https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228834641 (218M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 218.23M  15.1MB/s    in 62s     \n",
            "\n",
            "2025-03-19 16:56:46 (3.50 MB/s) - ‘spark-3.1.2-bin-hadoop3.2.tgz’ saved [228834641/228834641]\n",
            "\n",
            "total 223480\n",
            "drwxr-xr-x 1 root root      4096 Mar 17 13:32 sample_data\n",
            "-rw-r--r-- 1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "total 223484\n",
            "drwxr-xr-x  1 root root      4096 Mar 17 13:32 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24  2021 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n",
            "SPARK_HOME is set to: /content/spark-3.1.2-bin-hadoop3.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Java gateway process exited before sending its port number",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-88114b53b88d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q pyspark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TestSpark\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spark session created successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop3.2/python/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending its port number"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access your datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUkvIAwksZBy",
        "outputId": "e4288e03-1a4c-4b03-ea4c-e617eadfb11c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAMPLE FILE"
      ],
      "metadata": {
        "id": "wkRWtlEX1sGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create sample data with an 'id' column and a 'text_column'\n",
        "data_dict = {\n",
        "    'id': [1, 2, 3, 4, 5],\n",
        "    'text_column': [\n",
        "        \"This is a sample text.\",\n",
        "        \"This text is another example.\",\n",
        "        \"Sample text with duplicate sample text.\",\n",
        "        \"Another row with unique content.\",\n",
        "        \"This text contains some noise, e.g., punctuation!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = pd.DataFrame(data_dict)\n",
        "\n",
        "# Save the DataFrame to a CSV file named 'sample.csv' in the current directory\n",
        "csv_file_path = 'sample.csv'\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Display the DataFrame and confirm the CSV file generation\n",
        "print(\"Sample CSV generated successfully:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCrYgP0Ustxd",
        "outputId": "a3049308-542f-47e1-9369-8fa24eff8e0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample CSV generated successfully:\n",
            "   id                                        text_column\n",
            "0   1                             This is a sample text.\n",
            "1   2                      This text is another example.\n",
            "2   3            Sample text with duplicate sample text.\n",
            "3   4                   Another row with unique content.\n",
            "4   5  This text contains some noise, e.g., punctuation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PROCESSING"
      ],
      "metadata": {
        "id": "Eqp41zyM1ut7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas to work with CSV files\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to your CSV file stored in Google Drive\n",
        "csv_file_path = '/content/sample.csv'  # update this path if needed\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "data = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Display the first few rows to verify the data\n",
        "print(\"Sample data from CSV:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LbbEsl6sfxG",
        "outputId": "e8fd8ad5-3541-4936-dc92-d717bc688659"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data from CSV:\n",
            "   id                                        text_column\n",
            "0   1                             This is a sample text.\n",
            "1   2                      This text is another example.\n",
            "2   3            Sample text with duplicate sample text.\n",
            "3   4                   Another row with unique content.\n",
            "4   5  This text contains some noise, e.g., punctuation!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the missing punkt_tab resource\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U0R3sVFtM8D",
        "outputId": "b7f619e4-5323-45b4-b04e-90d4674d164d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import nltk and download the tokenizer models\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define a function to clean and tokenize text\n",
        "def clean_and_tokenize(text):\n",
        "    # Convert to lowercase and strip leading/trailing whitespace\n",
        "    text = text.lower().strip()\n",
        "    # Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Assuming the CSV has a column named 'text_column'\n",
        "# Apply the cleaning and tokenization to create a new 'tokens' column\n",
        "data['tokens'] = data['text_column'].apply(clean_and_tokenize)\n",
        "\n",
        "# Display the updated DataFrame with the new 'tokens' column\n",
        "print(\"Data after tokenization:\")\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ysj8M5ks3Nq",
        "outputId": "773dd9c1-5e74-4523-c7b9-52bbdb1c09b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after tokenization:\n",
            "   id                                        text_column  \\\n",
            "0   1                             This is a sample text.   \n",
            "1   2                      This text is another example.   \n",
            "2   3            Sample text with duplicate sample text.   \n",
            "3   4                   Another row with unique content.   \n",
            "4   5  This text contains some noise, e.g., punctuation!   \n",
            "\n",
            "                                              tokens  \n",
            "0                     [this, is, a, sample, text, .]  \n",
            "1              [this, text, is, another, example, .]  \n",
            "2   [sample, text, with, duplicate, sample, text, .]  \n",
            "3           [another, row, with, unique, content, .]  \n",
            "4  [this, text, contains, some, noise, ,, e.g., ,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load sample CSV file (make sure sample.csv exists in the current directory)\n",
        "data = pd.read_csv('sample.csv')\n",
        "print(\"Original Data:\")\n",
        "print(data, \"\\n\")\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Duplicate Content Detection\n",
        "# -------------------------------\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "duplicates_df = detect_duplicates(data)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Formatting Errors Detection\n",
        "# -------------------------------\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "    - Starts with an uppercase letter.\n",
        "    - Ends with proper punctuation (., !, or ?).\n",
        "\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Conflicting Information Detection\n",
        "# -------------------------------\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Example conflict pairs include:\n",
        "      - ('increase', 'decrease')\n",
        "      - ('up', 'down')\n",
        "      - ('true', 'false')\n",
        "      - ('positive', 'negative')\n",
        "      - ('win', 'loss')\n",
        "      - ('hot', 'cold')\n",
        "\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Data Anomalies Detection\n",
        "# -------------------------------\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "    - Empty text.\n",
        "    - Text that is extremely short.\n",
        "    - Excessive punctuation (three or more punctuation marks in a row).\n",
        "    - A high ratio of non-alphanumeric characters compared to alphanumeric ones.\n",
        "\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    # Detect excessive punctuation (3+ consecutive punctuation marks)\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    # Check for a high ratio of non-alphanumeric to alphanumeric characters\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "# -------------------------------\n",
        "# Apply All Detection Functions\n",
        "# -------------------------------\n",
        "# Apply formatting, conflicting information, and data anomalies detectors\n",
        "data['formatting_issues'] = data['text_column'].apply(detect_formatting_issues)\n",
        "data['conflicting_information'] = data['text_column'].apply(detect_conflicting_information)\n",
        "data['data_anomalies'] = data['text_column'].apply(detect_data_anomalies)\n",
        "\n",
        "# -------------------------------\n",
        "# Output the Results\n",
        "# -------------------------------\n",
        "print(\"Detected Duplicate Entries:\")\n",
        "print(duplicates_df, \"\\n\")\n",
        "\n",
        "print(\"Data with Inconsistency Issues Detected:\")\n",
        "print(data[['id', 'text_column', 'formatting_issues', 'conflicting_information', 'data_anomalies']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymljYeh7tfo8",
        "outputId": "f3b19550-afa0-45d3-bd76-f2617f821aea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   id                                        text_column\n",
            "0   1                             This is a sample text.\n",
            "1   2                      This text is another example.\n",
            "2   3            Sample text with duplicate sample text.\n",
            "3   4                   Another row with unique content.\n",
            "4   5  This text contains some noise, e.g., punctuation! \n",
            "\n",
            "Detected Duplicate Entries:\n",
            "Empty DataFrame\n",
            "Columns: [id, text_column]\n",
            "Index: [] \n",
            "\n",
            "Data with Inconsistency Issues Detected:\n",
            "   id                                        text_column formatting_issues  \\\n",
            "0   1                             This is a sample text.                []   \n",
            "1   2                      This text is another example.                []   \n",
            "2   3            Sample text with duplicate sample text.                []   \n",
            "3   4                   Another row with unique content.                []   \n",
            "4   5  This text contains some noise, e.g., punctuation!                []   \n",
            "\n",
            "  conflicting_information data_anomalies  \n",
            "0                      []             []  \n",
            "1                      []             []  \n",
            "2                      []             []  \n",
            "3                      []             []  \n",
            "4                      []             []  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING WITH CSV FILE"
      ],
      "metadata": {
        "id": "OyAaNnCW12s9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import io\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, FileLink, clear_output\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    # Capitalize first letter if not already\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    # Append period if missing ending punctuation\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Build the Upload & Processing Interface\n",
        "# -------------------------------\n",
        "\n",
        "# Create a file upload widget (accepting only CSV files)\n",
        "upload_widget = widgets.FileUpload(accept='.csv', multiple=False)\n",
        "\n",
        "# Create a process button widget\n",
        "process_button = widgets.Button(description=\"Process Document\")\n",
        "\n",
        "# Output area to display results and download link\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def process_file(button):\n",
        "    with output_area:\n",
        "        clear_output()  # Clear previous output\n",
        "        if upload_widget.value:\n",
        "            # Get the uploaded file (only one file allowed)\n",
        "            uploaded_filename = list(upload_widget.value.keys())[0]\n",
        "            content = upload_widget.value[uploaded_filename]['content']\n",
        "            # Read CSV from the uploaded file content\n",
        "            df = pd.read_csv(io.BytesIO(content))\n",
        "\n",
        "            print(\"Original Data:\")\n",
        "            print(df.head(), \"\\n\")\n",
        "\n",
        "            # Apply inconsistency detection on the 'text_column'\n",
        "            df['formatting_issues'] = df['text_column'].apply(detect_formatting_issues)\n",
        "            df['conflicting_information'] = df['text_column'].apply(detect_conflicting_information)\n",
        "            df['data_anomalies'] = df['text_column'].apply(detect_data_anomalies)\n",
        "\n",
        "            # Check for duplicate content\n",
        "            duplicates_df = detect_duplicates(df)\n",
        "            print(\"Detected Duplicate Entries:\")\n",
        "            if duplicates_df.empty:\n",
        "                print(\"No duplicate entries found.\\n\")\n",
        "            else:\n",
        "                print(duplicates_df, \"\\n\")\n",
        "\n",
        "            # Create a corrected text column for basic formatting corrections\n",
        "            df['corrected_text'] = df['text_column'].apply(correct_text)\n",
        "\n",
        "            print(\"Processed Data with Inconsistency Report:\")\n",
        "            display(df[['text_column', 'formatting_issues', 'conflicting_information', 'data_anomalies', 'corrected_text']])\n",
        "\n",
        "            # Save the corrected document to CSV\n",
        "            corrected_filename = \"corrected_document.csv\"\n",
        "            df.to_csv(corrected_filename, index=False)\n",
        "            print(\"\\nDownload the corrected document:\")\n",
        "            display(FileLink(corrected_filename))\n",
        "        else:\n",
        "            print(\"No file uploaded. Please upload a CSV file.\")\n",
        "\n",
        "# Bind the process_file function to the button click event\n",
        "process_button.on_click(process_file)\n",
        "\n",
        "# Display the interface: file upload widget, process button, and output area\n",
        "display(widgets.VBox([upload_widget, process_button, output_area]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3446d2c716f743f0a40e39e9518e8643",
            "045117c6ae9c465495cce7728d73beb5",
            "b5e884075dfc4b97abb7c2c164df936f",
            "2e8c65c3bbfe405aad31f2d15961d05c",
            "cd52c45010664579a6e8c6e3b6aceffe",
            "1143c6d61d51498f8f4361b3fec76737",
            "1f3b051bb96d4d63808b0b434cdfdc31",
            "82eef6bcfb5048e599923fa492e50f14",
            "cbf0f35359574cc08c306b2c0a1922e4",
            "90622235dde94d75895e107f664e48ae"
          ]
        },
        "id": "8MaiIbqPu9DI",
        "outputId": "0db8708e-60cf-4bc7-82cd-f27d060765ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(FileUpload(value={}, accept='.csv', description='Upload'), Button(description='Process Document…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3446d2c716f743f0a40e39e9518e8643"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz8acMRD6ycX",
        "outputId": "9abd71d8-4cf8-4179-fd54-3578bdee14c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.22.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.22.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.22.0 gradio-client-1.8.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# -------------------------------\n",
        "# Inconsistency Detection Functions\n",
        "# -------------------------------\n",
        "\n",
        "def detect_formatting_issues(text):\n",
        "    \"\"\"\n",
        "    Checks whether the text:\n",
        "      - Starts with an uppercase letter.\n",
        "      - Ends with proper punctuation (., !, or ?).\n",
        "    Returns a list of formatting issues.\n",
        "    \"\"\"\n",
        "    issues = []\n",
        "    if not text:\n",
        "        issues.append(\"Empty text\")\n",
        "        return issues\n",
        "    if not text[0].isupper():\n",
        "        issues.append(\"Does not start with an uppercase letter\")\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        issues.append(\"Does not end with proper punctuation\")\n",
        "    return issues\n",
        "\n",
        "def detect_conflicting_information(text):\n",
        "    \"\"\"\n",
        "    Checks for pairs of contradictory keywords in the text.\n",
        "    Returns a list of detected conflicts.\n",
        "    \"\"\"\n",
        "    conflict_pairs = [\n",
        "        ('increase', 'decrease'),\n",
        "        ('up', 'down'),\n",
        "        ('true', 'false'),\n",
        "        ('positive', 'negative'),\n",
        "        ('win', 'loss'),\n",
        "        ('hot', 'cold')\n",
        "    ]\n",
        "    conflicts = []\n",
        "    text_lower = text.lower()\n",
        "    for w1, w2 in conflict_pairs:\n",
        "        if w1 in text_lower and w2 in text_lower:\n",
        "            conflicts.append(f\"Conflict: {w1} vs {w2}\")\n",
        "    return conflicts\n",
        "\n",
        "def detect_data_anomalies(text):\n",
        "    \"\"\"\n",
        "    Detects data anomalies such as:\n",
        "      - Empty text.\n",
        "      - Text that is extremely short.\n",
        "      - Excessive punctuation (three or more punctuation marks in a row).\n",
        "      - A high ratio of non-alphanumeric to alphanumeric characters.\n",
        "    Returns a list of anomaly issues.\n",
        "    \"\"\"\n",
        "    anomalies = []\n",
        "    if not text or len(text.strip()) == 0:\n",
        "        anomalies.append(\"Empty text anomaly\")\n",
        "    if len(text) < 5:\n",
        "        anomalies.append(\"Text too short\")\n",
        "    if re.search(r'[\\!\\?\\.]{3,}', text):\n",
        "        anomalies.append(\"Excessive punctuation\")\n",
        "    alpha_numeric = re.findall(r'[a-zA-Z0-9]', text)\n",
        "    non_alphanumeric = re.findall(r'[^a-zA-Z0-9\\s]', text)\n",
        "    if alpha_numeric and (len(non_alphanumeric) / len(alpha_numeric)) > 0.5:\n",
        "        anomalies.append(\"High non-alphanumeric ratio\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_duplicates(df, text_column='text_column'):\n",
        "    \"\"\"\n",
        "    Returns a DataFrame with rows that have duplicate values in the specified text column.\n",
        "    \"\"\"\n",
        "    duplicates = df[df.duplicated(subset=[text_column], keep=False)]\n",
        "    return duplicates\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"\n",
        "    Corrects formatting issues:\n",
        "      - Capitalizes the first character if necessary.\n",
        "      - Ensures the text ends with a punctuation mark (adds a period if missing).\n",
        "    Returns the corrected text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return text\n",
        "    if not text[0].isupper():\n",
        "        text = text[0].upper() + text[1:]\n",
        "    if text[-1] not in ['.', '!', '?']:\n",
        "        text = text + '.'\n",
        "    return text\n",
        "\n",
        "# -------------------------------\n",
        "# Hadoop Integration Function\n",
        "# -------------------------------\n",
        "def upload_to_hdfs(local_file, hdfs_path):\n",
        "    \"\"\"\n",
        "    Uploads a local file to HDFS using a shell command.\n",
        "    Requires Hadoop to be installed and HDFS running.\n",
        "    \"\"\"\n",
        "    command = f\"hdfs dfs -put -f {local_file} {hdfs_path}\"\n",
        "    os.system(command)\n",
        "    return f\"File uploaded to HDFS at: {hdfs_path}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Gradio Interface Processing Function with Spark & Hadoop\n",
        "# -------------------------------\n",
        "\n",
        "def process_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return pd.DataFrame(), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    ext = os.path.splitext(file_obj.name)[1].lower()\n",
        "\n",
        "    # Initialize Spark session\n",
        "    spark = SparkSession.builder.master(\"local[*]\").appName(\"InconsistencyDetection\").getOrCreate()\n",
        "\n",
        "    try:\n",
        "        if ext == \".csv\":\n",
        "            spark_df = spark.read.option(\"header\", \"true\").csv(file_obj.name)\n",
        "        elif ext in [\".txt\"]:\n",
        "            spark_df = spark.read.text(file_obj.name)\n",
        "            spark_df = spark_df.withColumnRenamed(\"value\", \"text_column\")\n",
        "        elif ext in [\".json\"]:\n",
        "            try:\n",
        "                spark_df = spark.read.json(file_obj.name)\n",
        "                if \"text_column\" not in spark_df.columns:\n",
        "                    with open(file_obj.name, 'r', encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                    if isinstance(data, list) and all(isinstance(x, str) for x in data):\n",
        "                        spark_df = spark.createDataFrame([(x,) for x in data], [\"text_column\"])\n",
        "                    else:\n",
        "                        return pd.DataFrame({\"Error\": [\"JSON file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading JSON file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        elif ext in [\".xml\"]:\n",
        "            try:\n",
        "                df = pd.read_xml(file_obj.name)\n",
        "                if 'text_column' not in df.columns:\n",
        "                    return pd.DataFrame({\"Error\": [\"XML file does not contain a 'text_column'.\"]}), pd.DataFrame(), None, \"\", None\n",
        "                spark_df = spark.createDataFrame(df)\n",
        "            except Exception as e:\n",
        "                return pd.DataFrame({\"Error\": [f\"Error reading XML file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "        else:\n",
        "            return pd.DataFrame({\"Error\": [\"Unsupported file format. Supported formats: CSV, TXT, JSON, XML.\"]}), pd.DataFrame(), None, \"\", None\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame({\"Error\": [f\"Error reading file: {e}\"]}), pd.DataFrame(), None, \"\", None\n",
        "\n",
        "    # Define Spark UDFs wrapping our Python functions\n",
        "    detect_formatting_udf = udf(lambda x: detect_formatting_issues(x), ArrayType(StringType()))\n",
        "    detect_conflict_udf = udf(lambda x: detect_conflicting_information(x), ArrayType(StringType()))\n",
        "    detect_anomalies_udf = udf(lambda x: detect_data_anomalies(x), ArrayType(StringType()))\n",
        "    correct_text_udf = udf(lambda x: correct_text(x), StringType())\n",
        "\n",
        "    # Apply UDFs to create new columns\n",
        "    spark_df = spark_df.withColumn(\"formatting_issues\", detect_formatting_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"conflicting_information\", detect_conflict_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"data_anomalies\", detect_anomalies_udf(spark_df[\"text_column\"]))\n",
        "    spark_df = spark_df.withColumn(\"corrected_text\", correct_text_udf(spark_df[\"text_column\"]))\n",
        "\n",
        "    # Convert the Spark DataFrame to a Pandas DataFrame for further processing\n",
        "    pdf = spark_df.toPandas()\n",
        "\n",
        "    # Duplicate detection using our pandas function\n",
        "    duplicates_df = detect_duplicates(pdf)\n",
        "    if duplicates_df.empty:\n",
        "        duplicate_report = pd.DataFrame({\"Message\": [\"No duplicate entries found.\"]})\n",
        "    else:\n",
        "        duplicate_report = duplicates_df.copy()\n",
        "\n",
        "    processed_preview = pdf[['text_column', 'formatting_issues',\n",
        "                             'conflicting_information', 'data_anomalies',\n",
        "                             'corrected_text']]\n",
        "\n",
        "    # Save the corrected DataFrame as a downloadable CSV file\n",
        "    corrected_filename = \"corrected_document.csv\"\n",
        "    pdf.to_csv(corrected_filename, index=False)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Hadoop: Upload Corrected File to HDFS\n",
        "    # -------------------------------\n",
        "    # Adjust the HDFS path based on your Hadoop configuration\n",
        "    hdfs_destination = \"/user/your_username/corrected_document.csv\"\n",
        "    hadoop_status = upload_to_hdfs(corrected_filename, hdfs_destination)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Analysis & Graph Generation\n",
        "    # -------------------------------\n",
        "    total_records = len(pdf)\n",
        "    records_with_errors = pdf.apply(lambda row: len(row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']) > 0, axis=1).sum()\n",
        "\n",
        "    issues_counts = {}\n",
        "    for idx, row in pdf.iterrows():\n",
        "        issues = row['formatting_issues'] + row['conflicting_information'] + row['data_anomalies']\n",
        "        for issue in issues:\n",
        "            issues_counts[issue] = issues_counts.get(issue, 0) + 1\n",
        "\n",
        "    analysis_text = f\"Analysis Summary:\\nTotal records: {total_records}\\n\"\n",
        "    analysis_text += f\"Records with at least one inconsistency: {records_with_errors}\\n\"\n",
        "    analysis_text += \"Breakdown of inconsistency types:\\n\"\n",
        "    for issue, count in issues_counts.items():\n",
        "        analysis_text += f\" - {issue}: {count} occurrence(s)\\n\"\n",
        "    analysis_text += f\"\\nHadoop Upload Status: {hadoop_status}\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    if issues_counts:\n",
        "        x_vals = list(issues_counts.keys())\n",
        "        y_vals = list(issues_counts.values())\n",
        "        ax.plot(x_vals, y_vals, marker='o', linestyle='-', color='teal')\n",
        "        ax.set_title(\"Trend of Detected Inconsistencies\")\n",
        "        ax.set_xlabel(\"Inconsistency Type\")\n",
        "        ax.set_ylabel(\"Count\")\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, 'No inconsistencies detected', horizontalalignment='center',\n",
        "                verticalalignment='center', transform=ax.transAxes, fontsize=12)\n",
        "        ax.set_axis_off()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return processed_preview, duplicate_report, corrected_filename, analysis_text, fig\n",
        "\n",
        "# -------------------------------\n",
        "# Custom CSS for Black Background Interface\n",
        "# -------------------------------\n",
        "custom_css = \"\"\"\n",
        "body {\n",
        "  background-color: #000;\n",
        "  color: #fff;\n",
        "}\n",
        ".container {\n",
        "  border: 1px solid #444;\n",
        "  border-radius: 10px;\n",
        "  padding: 20px;\n",
        "  background-color: #222;\n",
        "  margin: 10px;\n",
        "}\n",
        "h1, h2, h3, h4, h5, h6 {\n",
        "  color: #fff;\n",
        "  text-align: center;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -------------------------------\n",
        "# Build and Launch the Gradio Blocks Interface\n",
        "# -------------------------------\n",
        "with gr.Blocks(css=custom_css) as demo:\n",
        "    gr.Markdown(\"<h1>CSV Inconsistency Detection & Correction with Spark & Hadoop</h1>\")\n",
        "    gr.Markdown(\n",
        "        \"Upload a file containing a column named **'text_column'**. \"\n",
        "        \"This tool uses Apache Spark for data processing and Hadoop for file storage. \"\n",
        "        \"It detects formatting issues, conflicting information, and data anomalies, \"\n",
        "        \"and then produces a corrected file for download. \"\n",
        "        \"Supported formats: CSV, TXT, JSON, XML.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            file_input = gr.File(label=\"Upload File\", file_count=\"single\", file_types=['.csv', '.txt', '.json', '.xml'])\n",
        "            process_button = gr.Button(\"Process Document\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            # This widget provides the download link for the corrected CSV file.\n",
        "            download_csv = gr.File(label=\"Download Corrected CSV\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            processed_preview = gr.Dataframe(label=\"Processed Data Preview\")\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            duplicate_report = gr.Dataframe(label=\"Duplicate Report\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            analysis_summary = gr.Textbox(label=\"Analysis Summary\", lines=8)\n",
        "        with gr.Column(elem_classes=\"container\"):\n",
        "            plot_output = gr.Plot(label=\"Inconsistencies Trend Graph\")\n",
        "\n",
        "    process_button.click(\n",
        "        fn=process_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[processed_preview, duplicate_report, download_csv, analysis_summary, plot_output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, inline=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "yvdh9iBYyR_q",
        "outputId": "bb3fe981-76aa-4814-b407-5a4eb6b10b19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://32983b0c7ba9ffa82a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://32983b0c7ba9ffa82a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to Generate Document with Inconsistencies\n"
      ],
      "metadata": {
        "id": "66kjsrXjvPWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with intentional inconsistencies\n",
        "data = {\n",
        "    'id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'text_column': [\n",
        "        \"this is a sample text\",                         # Formatting: not capitalized, missing punctuation.\n",
        "        \"This text is another example\",                   # Formatting: missing ending punctuation.\n",
        "        \"Increase profits, decrease costs.\",              # Conflicting info: \"increase\" and \"decrease\".\n",
        "        \"Short\",                                          # Data anomaly: too short, might be considered too brief.\n",
        "        \"Unique content!!!\",                              # Formatting anomaly: excessive punctuation.\n",
        "        \"this text contains noise, e.g., punctuation\",    # Formatting: not capitalized, missing punctuation.\n",
        "        \"Sample text with duplicate sample text.\",        # Duplicate candidate.\n",
        "        \"Sample text with duplicate sample text.\",        # Duplicate candidate.\n",
        "        \"up and down\",                                    # Conflicting info: \"up\" and \"down\".\n",
        "        \"win loss\"                                        # Conflicting info: \"win\" and \"loss\", missing punctuation.\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the dictionary\n",
        "df_inconsistent = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_filename = \"inconsistent_sample.csv\"\n",
        "df_inconsistent.to_csv(csv_filename, index=False)\n",
        "\n",
        "# Display the DataFrame and confirm file creation\n",
        "print(\"Generated CSV with inconsistencies:\")\n",
        "print(df_inconsistent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvwUWUeGvVos",
        "outputId": "7bffe1c8-a1e9-4164-81a4-5b213b703b42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated CSV with inconsistencies:\n",
            "   id                                  text_column\n",
            "0   1                        this is a sample text\n",
            "1   2                 This text is another example\n",
            "2   3            Increase profits, decrease costs.\n",
            "3   4                                        Short\n",
            "4   5                            Unique content!!!\n",
            "5   6  this text contains noise, e.g., punctuation\n",
            "6   7      Sample text with duplicate sample text.\n",
            "7   8      Sample text with duplicate sample text.\n",
            "8   9                                  up and down\n",
            "9  10                                     win loss\n"
          ]
        }
      ]
    }
  ]
}